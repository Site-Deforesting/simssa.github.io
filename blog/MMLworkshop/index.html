<!DOCTYPE html>
<html lang="en-us">  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  <meta name="theme-color" content="#3E3F3A">
  <!-- Windows Phone -->
  <meta name="msapplication-navbutton-color" content="#3E3F3A">
  <!-- iOS Safari -->
  <meta name="apple-mobile-web-app-status-bar-style" content="#3E3F3A">
  <title>      Music and Machine Learning Workshop &middot; SIMSSA  </title>  <link rel="shortcut icon" type="image/ico" href="../../assets/favicon.png">  <!-- CSS -->
  <link rel="stylesheet" href="../../assets/css/main.css" />  <!-- <link rel="stylesheet" type="text/css" href="http://localhost:4000/css/bootstrap.css">
  <link rel="stylesheet" type="text/css" href="http://localhost:4000/css/simssa.css"> -->
</head>  <body id ="Site" class='layout-reverse theme-base-sm sidebar-overlay'>    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"><div class="navbar navbar-expand-sm navbar-default navbar-fixed-top" role="navigation">
        <div class="container" id="nav-container">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="">SIMSSA</a>
            </div>
            <div class="collapse navbar-collapse">
                <ul class="navbar-nav nav mr-auto">
<!--                     <li class="active"><a href="/">Home</a></li> -->                    <li><a href="../../about">About</a></li>
                    <li><a href="../../people">Participants</a></li>
                    <!-- <li><a href="/publications">Publications</a></li> -->                    <li class="dropdown">
                      <a class="dropdown-toggle" role="button" data-toggle="dropdown" href="#">Activities<span class="caret"></span></a>
                      <ul class="dropdown-menu" role="menu">                            <li><a href='../../activities/corpora-and-datasets/' target='_top' >Datasets and Corpora</a></li>                            <li><a href='../../activities/impact/' target='_top' >Our impact</a></li>                            <li><a href='../../activities/media/' target='_top' >Media</a></li>                            <li><a href='../../activities/presentations/' target='_top' >Presentations</a></li>                            <li><a href='../../activities/publications/' target='_top' >Publications</a></li>                            <li><a href='../../activities/workshops/' target='_top' >Workshops</a></li>                      </ul>
                    </li>                    <li class="dropdown">
                      <a class="dropdown-toggle" role="button" data-toggle="dropdown" href="#">Projects and links<span class="caret"></span></a>
                      <ul class="dropdown-menu" role="menu">                            <li><a href='http://cantus.simssa.ca/' target='_top' >Cantus Ultimus</a></li>                            <li><a href='' target='_top' >ELVIS Project</a></li>                            <li><a href='http://jmir.sourceforge.net/index_jSymbolic.html' target='_top' >jSymbolic2</a></li>                            <li><a href='http://liber.simssa.ca' target='_top' >Liber Usualis</a></li>                            <li><a href='https://github.com/DDMAL/Andrew-Hughes-Chant' target='_top' >LMLO</a></li>                            <li><a href='' target='_top' >Musiclibs</a></li>                            <li><a href='http://ddmal.teamwork.com/' target='_top' >Teamwork</a></li>                        <hr id="menu-divider">
                        <li><a href="http://github.com/DDMAL" target="_blank">Github</a></li>
                        <li><a href="http://twitter.com/simssaproject" target="_blank">Twitter</a></li>
                      </ul>
                    </li>                    <li><a href="../../blog">Blog</a></li>
                    <li><a href="../../opportunities">Opportunities</a></li>                    <li><a href="../../contact">Contact us</a></li>                </ul>
            </div>
        </div>
    </div>        <div class="post">
  <div class="container">
    <div class="page-header" id="banner">
        <div class="row">
            <div class="col-lg-12">
            </div>
        </div>
    </div>
    <div class="row">
        <div class="col-lg-12">
        	<h2>Music and Machine Learning Workshop</h2><br />
        	<p class="blog-post">
        	   Posted by ehopkins on December 22, 2021
          </p><br />
			    <p><p>On 15 December 2021, the 14th annual <a href="https://sites.google.com/view/mml-2021/home">Music and Machine Learning Workshop</a> was held online, hosted by Rafael Ramirez (Universitat Pompeu Fabra, Spain), Darrell Conklin (University of the Basque Country, Spain), and José Manuel Iñesta (University of Alicante, Spain). This blog post is a really brief overview of the presentations; for more check out the
<a href="https://ehubox.ehu.eus/s/qQXQzLqPtg792RM">Proceedings</a> online.</p><p>José Manual Iñesta (pictured below) is also a visiting professor here at McGill this year, and a few members of our lab participated in the workshop too.</p><p><img src="../../assets/img/inesta.png" alt="" /></p><p>There were approximately 20 participants in the workshop, with topics including Roman Numeral Analysis, Optical Music Recognition, music demixing, and ways that music and machine learning draw on language, machine translation, and bioinformatics technologies. Each presentations was a quick 8 minutes followed by questions to give everyone a chance to catch up on each other’s research.</p><h3 id="multiscore-project-multimodal-transcription-of-music-scores">MultiScore Project: Multimodal Transcription of Music Scores</h3>
<p>Jorge Calvo-Zaragoza, A. Pertusa, A.J. Gallego, José M. Iñesta, L. Micó, J. Oncina, C. Pérez-Sancho, P-J. Ponce de León, D. Rizo</p><p>Jorge is an associate professor at the University of Alicante (and was formerly a SIMSSA postdoc!) He presented an overview of the scientific work projects of his teams at the University of Alicante, focusing on OMR (Optical Music Recognition) and AMT (Automatic Music Transcription) and ways to explore their commonalities. In both cases, polyphonic music poses special challenges. Multimodal Music Transcription is their project to attempt to exploit synergies between AMT and OMR and develop a free online transcription service.</p><p><img src="../../assets/img/Jorge2021-12-15at09.17.56.png" alt="" /></p><h3 id="3-d-motion-generation-for-double-bass-performance-from-musical-score">3-D motion generation for double bass performance from musical score</h3>
<p>Shinji Sako, Takeru Shirai</p><p>Shinji discussed work incorporating visual dimensions of musical performance, in this case creating a motion dataset for double bass performance. Bowing and other position info is tracked for separate body parts so tehy can generate bot performance motion and sound from a score. To compare human motion to the generated motion, they assessed the accuracy of the 3D position, the time variation, and conducted subjective analysis of the “naturalness” as assessed by double bass players.</p><p><img src="../../assets/img/Shinji2021-12-15at09.34.41.png" alt="" /></p><h3 id="playable-audio-texture-models">Playable Audio Texture Models</h3>
<p>Lonce Wyse, Chitralekha Gupta, Purnima Kamath</p><p>Lonce presented work on data-driven generative sound model design, looking at ways of combining and generating new sounds as well as making these new models playable. There were some great examples, including a “Trumpinet” where you can hear the real-time transformation of timbre as you move across the space, shown below.</p><p><img src="../../assets/img/Lonce2021-12-15at09.47.06.png" alt="" /></p><p>This <a href="https://animatedsound.com/arrhythmia2021/">example</a> demonstrates a similar concept but including different textures.</p><h3 id="music-demixing-with-the-slicq-transform">Music demixing with the sliCQ transform</h3>
<p>Sevag Hanssian</p><p>Sevag is a DDMAL Master’s student who shared his recent contribution to the <a href="https://www.aicrowd.com/challenges/music-demixing-challenge-ismir-2021">ISMIR satellite music demixing conference, MDX</a>. He gave an overview of some key principles and definitions for music demixing. Below, you can see how diferent sources have distinct spectral shapes (shown below), and that masks can be used to estimate them.</p><p><img src="../../assets/img/Sevag2021-12-15at09.54.54.png" alt="" /></p><p>He also covered the idea of a phase performance ceiling; ie, phase is routinely discarded in this work because it’s very hard to model. As an example, here is the mix compared to the vocals; the difference is very hard to see:
<img src="../../assets/img/Sevag2021-12-15at09.56.15.png" alt="" /></p><p>Check out <a href="https://github.com/sevagh/xumx-sliCQ">his entry for the competition on GitHub</a></p><h3 id="generating-expressive-features-of-music-performances-with-deep-sequence-models">Generating expressive features of music performances with deep sequence models</h3>
<p>Fabio Muneratti Ortega, Rafael Ramirez</p><p>Fabio presented recent work showing work on how to correlate score with audio using a model designed based in methods for language translation, as music scores also depend on sequence and context. Here is a look at the SkyNote software:</p><p><img src="../../assets/img/Fabio2021-12-15at10.05.54.png" alt="" /></p><h3 id="ctc-based-end-to-end-approach-for-full-page-optical-music-recognition">CTC-based end-to-end approach for full page Optical Music Recognition</h3>
<p>Antonio Ríos-Vila, Jorge Calvo-Zaragoza, José M. Iñesta</p><p>This presentation focused on attempts to move from a multi-stage process for OMR (where error accumulates at each successive step) to a CTC (Connectionist Temporal Classification)-based approach. They have had some success applying this technique with a synthetic situation corpus, and determined that they had some succes “when there is abundance of data and the corpora is of a printed nature”; future work will look at real-world scenarios with less data.</p><p><img src="../../assets/img/Antonio2021-12-15at10.22.18.png" alt="" /></p><h3 id="augmentednet-a-convolutional-recurrent-neural-network-for-automatic-roman-numeral-analysis-with-improved-data-augmentation">AugmentedNet: A Convolutional Recurrent Neural Network for Automatic Roman Numeral Analysis with Improved Data Augmentation</h3>
<p>Néstor Népoles López, Mark Gotham, and Ichiro Fujinaga</p><p><a href="https://napulen.github.io/">Néstor</a> presented a multi-task layout for Roman Numeral Analysis, identifyng new tasks to improve performance. Sepcifically, he presented on the development and use of synthetic training exmaples, using certain musical tricks to get closer to the rich texture of real music. In the example below, musical textures are added to the harmonic framework to make synthetic examples more musical:</p><p><img src="../../assets/img/Nestor2021-12-15at10.41.59.png" alt="" /></p><h3 id="error-modeling-and-correction-in-automatic-music-transcription-via-note-level-music-language-models">Error modeling and correction in Automatic Music Transcription via note-level Music Language Models</h3>
<p>Jose J. Valero-Mas, Andrew McLeod</p><p>This presentation focused on the automatic music transcription side of things, showing how Music Language Models can be used. The idea is that these models provide a canonical example that can be used as a basis for comparison to help with automatic detection of errors.</p><p><img src="../../assets/img/Jose2021-12-15at10.52.23.png" alt="" /></p><h3 id="an-unsupervised-domain-adaptation-framework-for-layout-analysis-of-music-score-images">An Unsupervised Domain Adaptation framework for Layout Analysis of Music Score Images</h3>
<p>Francisco J. Castellanos, Antonio Javier Gallego, Jorge Calvo-Zaragoza</p><p>In this presentation, we got an overview of ways to approach layout analysis for OMR by applying data from one domain (annotated manuscript) to a new one (not yet annotated manuscript) to speed up that process.</p><p><img src="../../assets/img/Paco2021-12-15at11.03.29.png" alt="" /></p><h3 id="error-detection-in-symbolic-music-for-omr-post-processing">Error Detection in Symbolic Music for OMR Post-Processing</h3>
<p>Timothy de Reuse, Ichiro Fujinaga</p><p>OMR makes lots of <em>unmusical</em> errors – can we automate the highlighting of these errors to make corection less tedious?</p><p>The work Tim presented is focused on detecting what looks wrong automatically to speed up correction. He uses the Needleman-Wunsch algorithm from bioinformatics to detect errors with sequence alignment.</p><p><img src="../../assets/img/Tim2021-12-15at11.14.38.png" alt="" /></p><p>Thanks to everyone who presented and all the best with your work in 2022!</p></p>
        </div>
    </div>
    <br /><hr /><br />
    <div class="related">
      <h2>Related Posts</h2>
      <ul class="related-posts">          <li>
            <h3>
              <a href="">
                Music and Machine Learning Workshop
                <!-- <small>22 Dec 2021</small> -->
              </a>
            </h3>
          </li>          <li>
            <h3>
              <a href="../ismir2019/">
                ISMIR 2019: Delft
                <!-- <small>16 Dec 2019</small> -->
              </a>
            </h3>
          </li>          <li>
            <h3>
              <a href="../simssa-xix/">
                SIMSSA XIX: Introducing DACT and MML16
                <!-- <small>11 Dec 2019</small> -->
              </a>
            </h3>
          </li>      </ul>
      <br /><br /><br>
    </div>
  </div>
</div>    <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script>
    <script type="text/javascript" src="../../js/bootstrap.min.js"></script>
  </body>
</html>

<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-06-09T15:05:34-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">SIMSSA</title><subtitle>A reserved &lt;a href=&quot;http://jekyllrb.com&quot; target=&quot;_blank&quot;&gt;Jekyll&lt;/a&gt; theme that places the utmost gravity on content with a hidden drawer. Made by &lt;a href=&quot;https://twitter.com/mdo&quot; target=&quot;_blank&quot;&gt;@mdo&lt;/a&gt;.</subtitle><author><name>Mark Otto</name><email>markdotto@gmail.com</email></author><entry><title type="html">Music and Machine Learning Workshop</title><link href="http://localhost:4000/blog/MMLworkshop/" rel="alternate" type="text/html" title="Music and Machine Learning Workshop" /><published>2021-12-22T00:00:00-05:00</published><updated>2021-12-22T00:00:00-05:00</updated><id>http://localhost:4000/blog/MMLworkshop</id><content type="html" xml:base="http://localhost:4000/blog/MMLworkshop/">&lt;p&gt;On 15 December 2021, the 14th annual &lt;a href=&quot;https://sites.google.com/view/mml-2021/home&quot;&gt;Music and Machine Learning Workshop&lt;/a&gt; was held online, hosted by Rafael Ramirez (Universitat Pompeu Fabra, Spain), Darrell Conklin (University of the Basque Country, Spain), and José Manuel Iñesta (University of Alicante, Spain). This blog post is a really brief overview of the presentations; for more check out the
&lt;a href=&quot;https://ehubox.ehu.eus/s/qQXQzLqPtg792RM&quot;&gt;Proceedings&lt;/a&gt; online.&lt;/p&gt;

&lt;p&gt;José Manual Iñesta (pictured below) is also a visiting professor here at McGill this year, and a few members of our lab participated in the workshop too.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/inesta.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There were approximately 20 participants in the workshop, with topics including Roman Numeral Analysis, Optical Music Recognition, music demixing, and ways that music and machine learning draw on language, machine translation, and bioinformatics technologies. Each presentations was a quick 8 minutes followed by questions to give everyone a chance to catch up on each other’s research.&lt;/p&gt;

&lt;h3 id=&quot;multiscore-project-multimodal-transcription-of-music-scores&quot;&gt;MultiScore Project: Multimodal Transcription of Music Scores&lt;/h3&gt;
&lt;p&gt;Jorge Calvo-Zaragoza, A. Pertusa, A.J. Gallego, José M. Iñesta, L. Micó, J. Oncina, C. Pérez-Sancho, P-J. Ponce de León, D. Rizo&lt;/p&gt;

&lt;p&gt;Jorge is an associate professor at the University of Alicante (and was formerly a SIMSSA postdoc!) He presented an overview of the scientific work projects of his teams at the University of Alicante, focusing on OMR (Optical Music Recognition) and AMT (Automatic Music Transcription) and ways to explore their commonalities. In both cases, polyphonic music poses special challenges. Multimodal Music Transcription is their project to attempt to exploit synergies between AMT and OMR and develop a free online transcription service.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/Jorge2021-12-15at09.17.56.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;3-d-motion-generation-for-double-bass-performance-from-musical-score&quot;&gt;3-D motion generation for double bass performance from musical score&lt;/h3&gt;
&lt;p&gt;Shinji Sako, Takeru Shirai&lt;/p&gt;

&lt;p&gt;Shinji discussed work incorporating visual dimensions of musical performance, in this case creating a motion dataset for double bass performance. Bowing and other position info is tracked for separate body parts so tehy can generate bot performance motion and sound from a score. To compare human motion to the generated motion, they assessed the accuracy of the 3D position, the time variation, and conducted subjective analysis of the “naturalness” as assessed by double bass players.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/Shinji2021-12-15at09.34.41.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;playable-audio-texture-models&quot;&gt;Playable Audio Texture Models&lt;/h3&gt;
&lt;p&gt;Lonce Wyse, Chitralekha Gupta, Purnima Kamath&lt;/p&gt;

&lt;p&gt;Lonce presented work on data-driven generative sound model design, looking at ways of combining and generating new sounds as well as making these new models playable. There were some great examples, including a “Trumpinet” where you can hear the real-time transformation of timbre as you move across the space, shown below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/Lonce2021-12-15at09.47.06.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This &lt;a href=&quot;https://animatedsound.com/arrhythmia2021/&quot;&gt;example&lt;/a&gt; demonstrates a similar concept but including different textures.&lt;/p&gt;

&lt;h3 id=&quot;music-demixing-with-the-slicq-transform&quot;&gt;Music demixing with the sliCQ transform&lt;/h3&gt;
&lt;p&gt;Sevag Hanssian&lt;/p&gt;

&lt;p&gt;Sevag is a DDMAL Master’s student who shared his recent contribution to the &lt;a href=&quot;https://www.aicrowd.com/challenges/music-demixing-challenge-ismir-2021&quot;&gt;ISMIR satellite music demixing conference, MDX&lt;/a&gt;. He gave an overview of some key principles and definitions for music demixing. Below, you can see how diferent sources have distinct spectral shapes (shown below), and that masks can be used to estimate them.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/Sevag2021-12-15at09.54.54.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;He also covered the idea of a phase performance ceiling; ie, phase is routinely discarded in this work because it’s very hard to model. As an example, here is the mix compared to the vocals; the difference is very hard to see:
&lt;img src=&quot;http://localhost:4000/assets/img/Sevag2021-12-15at09.56.15.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Check out &lt;a href=&quot;https://github.com/sevagh/xumx-sliCQ&quot;&gt;his entry for the competition on GitHub&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;generating-expressive-features-of-music-performances-with-deep-sequence-models&quot;&gt;Generating expressive features of music performances with deep sequence models&lt;/h3&gt;
&lt;p&gt;Fabio Muneratti Ortega, Rafael Ramirez&lt;/p&gt;

&lt;p&gt;Fabio presented recent work showing work on how to correlate score with audio using a model designed based in methods for language translation, as music scores also depend on sequence and context. Here is a look at the SkyNote software:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/Fabio2021-12-15at10.05.54.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;ctc-based-end-to-end-approach-for-full-page-optical-music-recognition&quot;&gt;CTC-based end-to-end approach for full page Optical Music Recognition&lt;/h3&gt;
&lt;p&gt;Antonio Ríos-Vila, Jorge Calvo-Zaragoza, José M. Iñesta&lt;/p&gt;

&lt;p&gt;This presentation focused on attempts to move from a multi-stage process for OMR (where error accumulates at each successive step) to a CTC (Connectionist Temporal Classification)-based approach. They have had some success applying this technique with a synthetic situation corpus, and determined that they had some succes “when there is abundance of data and the corpora is of a printed nature”; future work will look at real-world scenarios with less data.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/Antonio2021-12-15at10.22.18.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;augmentednet-a-convolutional-recurrent-neural-network-for-automatic-roman-numeral-analysis-with-improved-data-augmentation&quot;&gt;AugmentedNet: A Convolutional Recurrent Neural Network for Automatic Roman Numeral Analysis with Improved Data Augmentation&lt;/h3&gt;
&lt;p&gt;Néstor Népoles López, Mark Gotham, and Ichiro Fujinaga&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://napulen.github.io/&quot;&gt;Néstor&lt;/a&gt; presented a multi-task layout for Roman Numeral Analysis, identifyng new tasks to improve performance. Sepcifically, he presented on the development and use of synthetic training exmaples, using certain musical tricks to get closer to the rich texture of real music. In the example below, musical textures are added to the harmonic framework to make synthetic examples more musical:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/Nestor2021-12-15at10.41.59.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;error-modeling-and-correction-in-automatic-music-transcription-via-note-level-music-language-models&quot;&gt;Error modeling and correction in Automatic Music Transcription via note-level Music Language Models&lt;/h3&gt;
&lt;p&gt;Jose J. Valero-Mas, Andrew McLeod&lt;/p&gt;

&lt;p&gt;This presentation focused on the automatic music transcription side of things, showing how Music Language Models can be used. The idea is that these models provide a canonical example that can be used as a basis for comparison to help with automatic detection of errors.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/Jose2021-12-15at10.52.23.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;an-unsupervised-domain-adaptation-framework-for-layout-analysis-of-music-score-images&quot;&gt;An Unsupervised Domain Adaptation framework for Layout Analysis of Music Score Images&lt;/h3&gt;
&lt;p&gt;Francisco J. Castellanos, Antonio Javier Gallego, Jorge Calvo-Zaragoza&lt;/p&gt;

&lt;p&gt;In this presentation, we got an overview of ways to approach layout analysis for OMR by applying data from one domain (annotated manuscript) to a new one (not yet annotated manuscript) to speed up that process.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/Paco2021-12-15at11.03.29.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;error-detection-in-symbolic-music-for-omr-post-processing&quot;&gt;Error Detection in Symbolic Music for OMR Post-Processing&lt;/h3&gt;
&lt;p&gt;Timothy de Reuse, Ichiro Fujinaga&lt;/p&gt;

&lt;p&gt;OMR makes lots of &lt;em&gt;unmusical&lt;/em&gt; errors – can we automate the highlighting of these errors to make corection less tedious?&lt;/p&gt;

&lt;p&gt;The work Tim presented is focused on detecting what looks wrong automatically to speed up correction. He uses the Needleman-Wunsch algorithm from bioinformatics to detect errors with sequence alignment.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/Tim2021-12-15at11.14.38.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Thanks to everyone who presented and all the best with your work in 2022!&lt;/p&gt;</content><author><name>ehopkins</name></author><summary type="html">On 15 December 2021, the 14th annual Music and Machine Learning Workshop was held online, hosted by Rafael Ramirez (Universitat Pompeu Fabra, Spain), Darrell Conklin (University of the Basque Country, Spain), and José Manuel Iñesta (University of Alicante, Spain). This blog post is a really brief overview of the presentations; for more check out the Proceedings online.</summary></entry><entry><title type="html">ISMIR 2019: Delft</title><link href="http://localhost:4000/blog/ismir2019/" rel="alternate" type="text/html" title="ISMIR 2019: Delft" /><published>2019-12-16T00:00:00-05:00</published><updated>2019-12-16T00:00:00-05:00</updated><id>http://localhost:4000/blog/ismir2019</id><content type="html" xml:base="http://localhost:4000/blog/ismir2019/">&lt;p&gt;Back at the beginning of November, several members of our lab made the journey to the Netherlands to attend and present at &lt;a href=&quot;https://ismir2019.ewi.tudelft.nl/&quot;&gt;ISMIR2019&lt;/a&gt; and related satellite conferences. (ISMIR was from Nov. 4-8 with satellites before and after). Delft was a really neat city to visit, with as many bicycles, canals, and pounds of black licorice you could ever want…&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/2019_ismir_licorice.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We will be hosting &lt;a href=&quot;https://ismir.github.io/ISMIR2020/&quot;&gt;ISMIR2020&lt;/a&gt; here in Montreal so it was also great to see the ever-growing conference in action! (It was my first ISMIR too.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/2019_ismir_bicycles.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Satellites before the conference included WoRMS, WiMIR, and the first-ever Workshop on Designing Human-Centric MIR systems. We had presenters at two of these, and a few of our students attended the WiMIR hackathon event as well.&lt;/p&gt;

&lt;h2 id=&quot;workshop-on-reading-music-systems-worms&quot;&gt;Workshop on Reading Music Systems (WoRMS)&lt;/h2&gt;

&lt;p&gt;This was the second year for WoRMS – former SIMSSA postdoc Jorge Calvo-Zaraogoza is one of the chairs. PhD student Tim DeReuse presented work on “Robust Transcript Alignment on Medieval Chant Manuscripts”. Check out the full &lt;a href=&quot;https://drive.google.com/file/d/1lomhWwpO00VHv8zlIKwUHWICvmFQ7sBD/view&quot;&gt;WoRMS proceedings&lt;/a&gt;, including Tim’s paper.&lt;/p&gt;

&lt;h2 id=&quot;designing-human-centric-mir-systems&quot;&gt;Designing Human-Centric MIR Systems&lt;/h2&gt;

&lt;p&gt;This was the first year of the &lt;a href=&quot;https://sites.google.com/view/designinghuman-centricmir/home&quot;&gt;Workshop on Designing Human-Centric MIR Systems&lt;/a&gt; Link to the [proceedings](https://sites.google.com/view/designinghuman-centricmir/proceedings.&lt;/p&gt;

&lt;p&gt;SIMSSA Postdoc Finn Upham presented “Human Subtracted: Social Distortion of Music Technology.” Check out their &lt;a href=&quot;http://localhost:4000/assets/files/upham_2019_hcmir_humansubtracted_slides.pdf&quot;&gt;slides&lt;/a&gt; and &lt;a href=&quot;http://localhost:4000/assets/files/upham_2019_hcmir_humansubtracted_extabstract.pdf&quot;&gt;extended abstract&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/dereuse_2019_worms.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;ismir&quot;&gt;ISMIR&lt;/h2&gt;

&lt;p&gt;ISMIR in Delft was bigger than ever, with over 500 participants. Two students from our lab gave presentations.&lt;/p&gt;

&lt;p&gt;Yaolong Ju presented “&lt;a href=&quot;http://archives.ismir.net/ismir2019/paper/000106.pdf&quot;&gt;An Interactive Workflow for Generating Chord Labels for Homorhythmic Music in Symbolic Formats&lt;/a&gt;.”&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/ju_2019_ismir.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Tim de Reuse presented again, this time on “&lt;a href=&quot;http://archives.ismir.net/ismir2019/paper/000093.pdf&quot;&gt;Pattern Clustering in Monophonic Music by Learning a Non-Linear Embedding From Human Annotations&lt;/a&gt;”. He was also the winner for the MIREX “Patterns for Prediction” task.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/dereuse_2019_ismir.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ISMIR2019 also included tutorials, an unconference, and nine different flavours of water available all day! They set a high bar for 2020 and it was a pleasure to attend.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/2019_ismir_cafe.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;digital-libraries-for-musicology-dlfm&quot;&gt;Digital Libraries for Musicology (DLfM)&lt;/h2&gt;

&lt;p&gt;After ISMIR, we headed to The Hague for the 6th edition of &lt;a href=&quot;https://dlfm.web.ox.ac.uk/&quot;&gt;Digital Libraries for Musicology&lt;/a&gt;, held at the National Library of the Netherlands. Our lab was well-represented, with four people presenting their research.&lt;/p&gt;

&lt;p&gt;Néstor Nápoles López discussed his work on “&lt;a href=&quot;http://localhost:4000/assets/files/napoleslopez_2019_dlfm_keyfinding_paper.pdf&quot;&gt;Key-Finding Based on a Hidden Markov Model and Key Profiles&lt;/a&gt;”, done in collaboration with former SIMSSA postdoc Claire Arthur. Check out the &lt;a href=&quot;http://localhost:4000/assets/files/napoleslopez_2019_dlfm_keyfinding_slides.pdf&quot;&gt;slides&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/napoleslopez_2019_dlfm.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Matan Gover presented “A notation-based query language  for searching in symbolic music”. Link to &lt;a href=&quot;http://localhost:4000/assets/files/gover_2019_dlfm_notation_slides.pdf&quot;&gt;slides&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/gover_2019_dlfm.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Emily Hopkins (me!) presented a poster on “SIMSSA DB: Symbolic Music Discovery and Search.”
Link to &lt;a href=&quot;http://localhost:4000/assets/files/hopkins_2019_dlfm_simssadb_paper.pdf&quot;&gt;short paper&lt;/a&gt; and &lt;a href=&quot;http://localhost:4000/assets/files/hopkins_2019_dlfm_simssadb_poster.pdf&quot;&gt;poster&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/hopkins_2019_dlfm.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Martha Thomae presented on “The Mensural Scoring-up Tool.” (See our &lt;a href=&quot;https://simssa.ca/blog/interviewing-martha/&quot;&gt;blog post&lt;/a&gt; for an earlier interview with her about this research.)
Link to &lt;a href=&quot;http://localhost:4000/assets/files/thomae_2019_dlfm_mensural_slides.pptx&quot;&gt;slides&lt;/a&gt; and &lt;a href=&quot;http://localhost:4000/assets/files/thomae_2019_dlfm_mensural_paper.pdf&quot;&gt;paper&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/thomae_2019_dlfm.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Hope to see you all at &lt;a href=&quot;https://ismir.github.io/ISMIR2020/&quot;&gt;ISMIR2020&lt;/a&gt;! You can follow us on Twitter too, &lt;a href=&quot;https://twitter.com/ismir2020&quot;&gt;@ismir2020&lt;/a&gt;. &lt;br /&gt;
See you next year!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/2019_ismir_canal.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;</content><author><name>ehopkins</name></author><summary type="html">Back at the beginning of November, several members of our lab made the journey to the Netherlands to attend and present at ISMIR2019 and related satellite conferences. (ISMIR was from Nov. 4-8 with satellites before and after). Delft was a really neat city to visit, with as many bicycles, canals, and pounds of black licorice you could ever want…</summary></entry><entry><title type="html">SIMSSA XIX: Introducing DACT and MML16</title><link href="http://localhost:4000/blog/simssa-xix/" rel="alternate" type="text/html" title="SIMSSA XIX: Introducing DACT and MML16" /><published>2019-12-11T00:00:00-05:00</published><updated>2019-12-11T00:00:00-05:00</updated><id>http://localhost:4000/blog/simssa-xix</id><content type="html" xml:base="http://localhost:4000/blog/simssa-xix/">&lt;p&gt;On 21 September 2019, we had our 19th SIMSSA workshop, this time here at McGill in collaboration with &lt;a href=&quot;https://www.cirmmt.org&quot;&gt;CIRMMT&lt;/a&gt;. This weekend coincided with a workshop for Julie Cumming’s new SSHRC Grant, Mapping the Musical Landscape of the 16th Century (MML16), as well as Jennifer Bain’s new partnership grant, Digital Analysis of Chant Transmission (DACT). Scholars involved in all three projects were here for a few days to make plans and exchange ideas, and the SIMSSA Workshop had around 45 attendees.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/simssaxix_group.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ichiro Fujinaga: Welcome &amp;amp; Introduction&lt;/strong&gt;&lt;br /&gt;
As always, Ich started the day with an overview of recent SIMSSA highlights, including workshops and other major projects.&lt;br /&gt;
(Slides: 
&lt;a href=&quot;http://localhost:4000/assets/files/fujinaga_2019_simssaxix_intro_slides.key&quot;&gt;Keynote&lt;/a&gt; 
and &lt;a href=&quot;http://localhost:4000/assets/files/fujinaga_2019_simssaxix_intro_slides.pdf&quot;&gt;PDF&lt;/a&gt;)
&lt;img src=&quot;http://localhost:4000/assets/img/fujinaga_simssaxix.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Jennifer Bain: The Digital Analysis of Chant Transmission (DACT)&lt;/strong&gt;&lt;br /&gt;
Jennifer presented on her new SSHRC Partnership DACT! Now that Cantus Ultimus has ended, we’re excited to see where this new venture goes.&lt;br /&gt;
(Slides: 
&lt;a href=&quot;http://localhost:4000/assets/files/bain_2019_simssaxix_dact_slides.pdf&quot;&gt;PDF&lt;/a&gt; and &lt;a href=&quot;http://localhost:4000/assets/files/bain_2019_simssaxix_dact_slides.pptx&quot;&gt;Powerpoint&lt;/a&gt;)
&lt;img src=&quot;http://localhost:4000/assets/img/bain__2019_simssaxix.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Rebecca Shaw: The Differentiae Database&lt;/strong&gt;&lt;br /&gt;
Rebecca Shaw presented on recent work she has done the Differentiae Database, which she originally presented at DLfM in 2018 (blog post)[https://simssa.ca/blog/DLfM-becky-shaw/.  &lt;br /&gt;
(Slides: 
&lt;a href=&quot;http://localhost:4000/assets/files/shaw_2019_simssaxix_differentiae_slides.pdf&quot;&gt;PDF&lt;/a&gt; 
and &lt;a href=&quot;http://localhost:4000/assets/files/shaw_2019_simssaxix_differentiae_slides.pptx&quot;&gt;Powerpoint&lt;/a&gt;)
&lt;img src=&quot;http://localhost:4000/assets/img/shaw_2019_simssaxix.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Néstor Nápoles López: Cantus Ultimus: Status Update&lt;/strong&gt;&lt;br /&gt;
Néstor has been helping with updates to the Cantus Ultimus site, helping improve the site infrastructure and working towards improving the infrastructure for manuscript upload.&lt;br /&gt;
(Slides: 
&lt;a href=&quot;http://localhost:4000/assets/files/napoleslopez_simssaxix_2019_cantus_slides.pdf&quot;&gt;PDF&lt;/a&gt;)
&lt;img src=&quot;http://localhost:4000/assets/img/napoleslopez_2019_simssaxix.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Evan Savage: Neume Component Pitch and Type Classification&lt;/strong&gt;&lt;br /&gt;
Evan breaks down the process for breaking neumes into components to identify their types and pitches.&lt;br /&gt;
(Slides: 
&lt;a href=&quot;http://localhost:4000/assets/files/savage_2019_simssaxix_neume_slides.pdf&quot;&gt;PDF&lt;/a&gt; 
and &lt;a href=&quot;http://localhost:4000/assets/files/savage_2019_simssaxix_neume_slides.pptx&quot;&gt;Powerpoint&lt;/a&gt;)
&lt;img src=&quot;http://localhost:4000/assets/img/savage_2019_simssaxix.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tim de Reuse: Text-to-Image Alignment on Chant Manuscripts&lt;/strong&gt;&lt;br /&gt;
Tim de Reuse has been working on aligning chant manuscript images with transcribed text using a combination of OCR and a sequence alignment algorithm developed for working with DNA.&lt;br /&gt;
(Slides: 
&lt;a href=&quot;http://localhost:4000/assets/files/dereuse_2019_simssaxix_alignment_slides.pdf&quot;&gt;PDF&lt;/a&gt; and &lt;a href=&quot;http://localhost:4000/assets/files/dereuse_2019_simssaxix_alignment_slides.pptx&quot;&gt;Powerpoint&lt;/a&gt;)
&lt;img src=&quot;http://localhost:4000/assets/img/dereuse_2019_simssaxix.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Juliette Regimbal &amp;amp; Caitlin Hutnyk: Neon: Full Manuscripts, Lyrics and Staves&lt;/strong&gt;&lt;br /&gt;
Juliette and Caitlin presented their progress on Neon, our online Neume Editing tool. They welcome feedback from users – &lt;a href=&quot;https://ddmal.music.mcgill.ca/Neon/&quot;&gt;try Neon&lt;/a&gt;! Caitlin was one of our new student hires from summer of 2019.&lt;br /&gt;
(Slides: 
&lt;a href=&quot;http://localhost:4000/assets/files/regimbal_2019_simssaxix_neon_slides.pdf&quot;&gt;PDF&lt;/a&gt;)
&lt;img src=&quot;http://localhost:4000/assets/img/regimbal_hutnyk_2019_simssaxix.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Imane Chafi: Cress: MEI Mapping Tool&lt;/strong&gt;&lt;br /&gt;
Imane was another one of our new summer 2019 student lab workers, and her main project was developing a tool for mapping individual researchers’ spreadsheets with neume labels on to the correct MEI representation, and making it easier to share these labels.&lt;br /&gt;
(Slides: 
&lt;a href=&quot;https://prezi.com/view/TQt8frGysmmRUSAMvI76/&quot;&gt;Prezi&lt;/a&gt;)
&lt;img src=&quot;http://localhost:4000/assets/img/chafi_2019_simssaxix.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Alex Daigle: Rodan 2019&lt;/strong&gt;&lt;br /&gt;
Alex has been working on updating our main workflow engine, Rodan.&lt;br /&gt;
(Slides: 
&lt;a href=&quot;http://localhost:4000/assets/files/daigle_2019_simssaxix_rodan_slides.pdf&quot;&gt;PDF&lt;/a&gt; 
and &lt;a href=&quot;http://localhost:4000/assets/files/daigle_2019_simssaxix_rodan_slides.pptx&quot;&gt;Powerpoint&lt;/a&gt;)
&lt;img src=&quot;http://localhost:4000/assets/img/daigle_2019_simssaxix.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Julie Cumming: Introducing MML16: Mapping the Musical Landscape of the 16th Century&lt;/strong&gt;&lt;br /&gt;
MML16 is Julie Cumming’s new SSHRC Insight grant, bringing together print and manuscript sources, online databases, and new ways of exploring the movement of music throughout this century.&lt;br /&gt;
(Slides: 
&lt;a href=&quot;http://localhost:4000/assets/files/cumming_2019_simssaxix_mml16_slides.pdf&quot;&gt;PDF&lt;/a&gt; 
and &lt;a href=&quot;http://localhost:4000/assets/files/cumming_2019_simssaxix_mml16_slides.pptx&quot;&gt;Powerpoint&lt;/a&gt;)
&lt;img src=&quot;http://localhost:4000/assets/img/cumming_2019_simssaxix_mml16.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Martha Thomae: OMR for Mensural Notation: Looking at a Guatemalan Music Manuscript&lt;/strong&gt;&lt;br /&gt;
Martha’s PhD dissertation involves going taking a choirbook manuscript in Guatemala through the digitization process all the way to an MEI score. She started by travelling to Guatemala and digitizing it herself and has now moved on to working on the technology required to do OMR for mensural notation.&lt;br /&gt;
(Slides: 
&lt;a href=&quot;http://localhost:4000/assets/files/thomae_2019_simssaxix_mensural_slides.pdf&quot;&gt;PDF&lt;/a&gt; 
and &lt;a href=&quot;http://localhost:4000/assets/files/thomae_2019_simssaxix_mensural_slides.pptx&quot;&gt;Powerpoint&lt;/a&gt;)
&lt;img src=&quot;http://localhost:4000/assets/img/thomae_2019_simssaxix.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Yaolong Ju (&amp;amp; Sam Howes): Chord Progressions in Lutheran Chorales&lt;/strong&gt;&lt;br /&gt;
As part of his work on harmonic analysis and chord labelling, Yaolong has been working with Sam on looking at chord progressions in chorales.&lt;br /&gt;
(Slides: 
&lt;a href=&quot;http://localhost:4000/assets/files/ju_2019_simssaxix_progressions_slides.pdf&quot;&gt;PDF&lt;/a&gt; 
and &lt;a href=&quot;http://localhost:4000/assets/files/ju_2019_simssaxix_progressions_slides.pptx&quot;&gt;Powerpoint&lt;/a&gt;)
&lt;img src=&quot;http://localhost:4000/assets/img/ju_2019_simssaxix.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Emily Hopkins: SIMSSA DB: Symbolic Music Discovery and Search&lt;/strong&gt;&lt;br /&gt;
Emily is part of the team working on the SIMSSA DB. She provided an overview of some of the key factors involved in modelling music for the DB and also making sure our data is available long-term.&lt;br /&gt;
(Slides: 
&lt;a href=&quot;http://localhost:4000/assets/files/hopkins_2019_simssaxix_simssadb_slides.pdf&quot;&gt;PDF&lt;/a&gt; 
and &lt;a href=&quot;http://localhost:4000/assets/files/hopkins_2019_simssaxix_simssadb_slides.pptx&quot;&gt;Powerpoint&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;David Garfinkle: PatternFinder: Symbolic Music Retrieval&lt;/strong&gt;&lt;br /&gt;
David provided an overview of the latest updates on his symbolic music search tool, including outlining future plans to integrate with the SIMSSA DB.&lt;br /&gt;
(Slides: 
&lt;a href=&quot;http://localhost:4000/assets/files/garfinkle_2019_simssaxix_patternfinder_slides.pdf&quot;&gt;PDF&lt;/a&gt; 
and &lt;a href=&quot;http://localhost:4000/assets/files/garfinkle_2019_simssaxix_patternfinder_slides.pptx&quot;&gt;Powerpoint&lt;/a&gt;)
&lt;img src=&quot;http://localhost:4000/assets/img/garfinkle_2019_simssaxix.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Julie Cumming, Cory McKay, Néstor Nápoles López, and Sylvain Margot: Contrapuntal Style: Pierre de la Rue vs. Josquin Des Prez&lt;/strong&gt;
This large team of researchers presented on their corpus study looking at differences in style between La Rue and Josquin, outlining various approaches.&lt;br /&gt;
(Slides: 
&lt;a href=&quot;http://localhost:4000/assets/files/cumming_2019_simssaxix_contrapuntal_slides.pdf&quot;&gt;PDF&lt;/a&gt; and &lt;a href=&quot;http://localhost:4000/assets/files/cumming_2019_simssaxix_contrapuntal_slides.pptx&quot;&gt;Powerpoint&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Cory McKay (on behalf of Rían Adamian): jSymbolic in 2019: Updates and Improvements&lt;/strong&gt;&lt;br /&gt;
Rían was also one of our summer students, working for Cory McKay on further developments for &lt;a href=&quot;http://jmir.sourceforge.net/jSymbolic.html&quot;&gt;jSymbolic.&lt;/a&gt;&lt;br /&gt;
(Slides: 
&lt;a href=&quot;http://localhost:4000/assets/files/mckay_2019_simssaxix_jsymbolic_slides.pdf&quot;&gt;PDF&lt;/a&gt; and &lt;a href=&quot;http://localhost:4000/assets/files/mckay_2019_simssaxix_jsymbolic_slides.pptx&quot;&gt;Powerpoint&lt;/a&gt;)
&lt;img src=&quot;http://localhost:4000/assets/img/mckay_2019_simssaxix.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Thank you to all of our guests, presenters, and those who helped organize and set up!&lt;/p&gt;</content><author><name>ehopkins</name></author><summary type="html">On 21 September 2019, we had our 19th SIMSSA workshop, this time here at McGill in collaboration with CIRMMT. This weekend coincided with a workshop for Julie Cumming’s new SSHRC Grant, Mapping the Musical Landscape of the 16th Century (MML16), as well as Jennifer Bain’s new partnership grant, Digital Analysis of Chant Transmission (DACT). Scholars involved in all three projects were here for a few days to make plans and exchange ideas, and the SIMSSA Workshop had around 45 attendees.</summary></entry><entry><title type="html">SIMSSA XVII: Infrastructure for Music Discovery</title><link href="http://localhost:4000/blog/simssa_xvii/" rel="alternate" type="text/html" title="SIMSSA XVII: Infrastructure for Music Discovery" /><published>2018-12-01T00:00:00-05:00</published><updated>2018-12-01T00:00:00-05:00</updated><id>http://localhost:4000/blog/simssa_xvii</id><content type="html" xml:base="http://localhost:4000/blog/simssa_xvii/">&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/group-shot-simssaxvii.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Earlier this month we had our seventeenth SIMSSA workshop, this time here at McGill in collaboration with &lt;a href=&quot;https://www.cirmmt.org&quot;&gt;CIRMMT&lt;/a&gt;). Sally Jo Cunningham gave a CIRMMT Distinguished Lecture earlier that week on &lt;a href=&quot;https://www.cirmmt.org/activities/distinguished-lectures/sally-jo-cunningham&quot;&gt;Engagement with Personal Music Collections&lt;/a&gt; which inspired our theme: Infrastructure for Music Discovery. We had 19 different speakers covering everything from OMR to library reference models to linked data to country music! Here is a roundup of all the presenter’s slides and some pictures from the event.&lt;/p&gt;

&lt;p&gt;To start the day, Ichiro Fujinaga gave us an overview of the project, including the most recent workshops and other activities. (Slides: &lt;a href=&quot;http://localhost:4000/assets/files/fujinaga1-simssaxvii.pdf&quot;&gt;PDF&lt;/a&gt;). After that, I (Emily Hopkins) gave an introduction to the different components of the SIMSSA project, explaining how they all come together in context. (Slides: &lt;a href=&quot;http://localhost:4000/assets/files/hopkins1-simssaxvii.pptx&quot;&gt;Powerpoint&lt;/a&gt; and &lt;a href=&quot;http://localhost:4000/assets/files/hopkins1-simssaxvii.pdf&quot;&gt;PDF&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;Next, we had several updates from students working in the DDMAL labs on various parts of our OMR processes. Minh Anh Nguyen presented her work on the &lt;a href=&quot;https://github.com/DDMAL/Interactive-Classifier&quot;&gt;Interactive Classifier&lt;/a&gt;. (Slides: &lt;a href=&quot;http://localhost:4000/assets/files/nguyen-simssaxvii.pptx&quot;&gt;Powerpoint&lt;/a&gt; and &lt;a href=&quot;http://localhost:4000/assets/files/nguyen-simssaxvii.pdf&quot;&gt;PDF&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/nguyen-simssaxvii.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Then, Noah Baxter gave us a look at his recent progress on the pitch finding part of the process, including &lt;a href=&quot;https://github.com/DDMAL/diagonal-neume-slicing&quot;&gt;diagonal neume slicing&lt;/a&gt; and converting &lt;a href=&quot;https://github.com/DDMAL/JSOMR2MEI&quot;&gt;JSON OMR files to MEI&lt;/a&gt;. (Slides: &lt;a href=&quot;(http://localhost:4000/assets/files/baxter-simssaxvii.pdf)&quot;&gt;PDF&lt;/a&gt;
&lt;img src=&quot;http://localhost:4000/assets/img/baxter-simssaxvii.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Next up is our ground truth image editor, &lt;a href=&quot;https://github.com/DDMAL/Pixel.js&quot;&gt;Pixel.js&lt;/a&gt; – originally developed by Zeyad Saleh and Ké Zhang, most recently developed by Eric Liu. Our postdoc Gabriel Vigliensoni gave this presentation. (Slides: &lt;a href=&quot;http://localhost:4000/assets/files/vigliensoni-simssaxvii.pdf&quot;&gt;PDF&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;After that, Tim De Reuse showed his most recent work developing a process for aligning musical texts with notation. (Slides: &lt;a href=&quot;http://localhost:4000/assets/files/de_reuse-simssaxvii.pptx&quot;&gt;Powerpoint&lt;/a&gt; and &lt;a href=&quot;http://localhost:4000/assets/files/de_reuse-simssaxvii.pdf&quot;&gt;PDF&lt;/a&gt;)
&lt;img src=&quot;http://localhost:4000/assets/img/de_reuse-simssaxvii.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Zoé McLennan and Juliette Regimbal presented their latest work on &lt;a href=&quot;https://github.com/DDMAL/Neon2&quot;&gt;Neon&lt;/a&gt;, a web-based neume editor that uses &lt;a href=&quot;http://www.verovio.org/index.xhtml&quot;&gt;Verovio&lt;/a&gt; to render the music. (Slides: &lt;a href=&quot;http://localhost:4000/assets/files/mclennan-regimbal-simssaxvii.pdf&quot;&gt;PDF&lt;/a&gt;)
&lt;img src=&quot;http://localhost:4000/assets/img/mclennan-simssaxvii.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;http://localhost:4000/assets/img/regimbal-simssaxvii.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Alex Daigle then gave us an overview of &lt;a href=&quot;https://github.com/DDMAL/Rodan&quot;&gt;Rodan&lt;/a&gt;, which is our web-based workflow manager for the OMR process. (Slides: &lt;a href=&quot;http://localhost:4000/assets/files/daigle-simssaxvii.pptx&quot;&gt;Powerpoint&lt;/a&gt; and &lt;a href=&quot;http://localhost:4000/assets/files/daigle-simssaxvii.pdf&quot;&gt;PDF&lt;/a&gt;)
&lt;img src=&quot;http://localhost:4000/assets/img/daigle-simssaxvii.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Néstor Nápoles showed off his recent work on the &lt;a href=&quot;https://github.com/DDMAL/CantusEditor&quot;&gt;Cantus Editor&lt;/a&gt;, working for &lt;a href=&quot;https://cantus.simssa.ca/&quot;&gt;Cantus Ultimus&lt;/a&gt;. He also showed off the &lt;a href=&quot;cantusindex.org&quot;&gt;Cantus Index&lt;/a&gt; site, which brings many different chant collections (including the &lt;a href=&quot;http://cantus.uwaterloo.ca/&quot;&gt;Cantus Database&lt;/a&gt;) together. (Slides: &lt;a href=&quot;http://localhost:4000/assets/files/napoles-simssaxvii.pdf&quot;&gt;PDF&lt;/a&gt;)
&lt;img src=&quot;http://localhost:4000/assets/img/napoles-simssaxvii.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://iiif.io/&quot;&gt;IIIF&lt;/a&gt; is already an important part of SIMSSA’s infrastructure, and we’ve also been working with the British Library on developing the &lt;a href=&quot;https://github.com/DDMAL/IIIF-AV-player&quot;&gt;IIIF AV player&lt;/a&gt; for the new IIIF audio/visual support. Andrew Kam demonstrated his work on the player. (Slides: &lt;a href=&quot;http://localhost:4000/assets/files/kam-simssaxvii.pptx&quot;&gt;Powerpoint&lt;/a&gt; and &lt;a href=&quot;http://localhost:4000/assets/files/kam-simssaxvii.pdf&quot;&gt;PDF&lt;/a&gt;)
&lt;img src=&quot;http://localhost:4000/assets/img/kam-simssaxvii.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Martha Thomae is doing her dissertation on applying our OMR processes as well as her &lt;a href=&quot;https://github.com/ELVIS-Project/scoring-up&quot;&gt;Scoring-Up tool&lt;/a&gt; to musical manuscripts from her home country of Guatemala. She gave us an overview of the process involved, including working with different organizations in Guatemala and learning how to photograph scores for preservation. (Slides: &lt;a href=&quot;http://localhost:4000/assets/files/thomae-simssaxvii.pptx&quot;&gt;Powerpoint&lt;/a&gt; and &lt;a href=&quot;http://localhost:4000/assets/files/thomae-simssaxvii.pdf&quot;&gt;PDF&lt;/a&gt;)
&lt;img src=&quot;http://localhost:4000/assets/img/thomae-simssaxvii.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Matan Gover developed a musical query language as part of a class with Ichiro last year, and demonstrated it for us today. Instead of using text to query music, why not use actual music? Seen here is Craig Sapp’s &lt;a href=&quot;http://verovio.humdrum.org/&quot;&gt;Verovio Humdrum Viewer&lt;/a&gt; in action. (Slides: &lt;a href=&quot;http://localhost:4000/assets/files/gover-simssaxvii.key&quot;&gt;Keynote&lt;/a&gt; and &lt;a href=&quot;http://localhost:4000/assets/files/gover-simssaxvii.pdf&quot;&gt;PDF&lt;/a&gt;)
&lt;img src=&quot;http://localhost:4000/assets/img/gover-simssaxvii.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;David Garfinkle talked about his work on &lt;a href=&quot;https://github.com/ELVIS-Project/PatternFinder&quot;&gt;PatternFinder&lt;/a&gt;, our melodic search technology. In addition to handling one-voice (monophonic) melodic search, David’s implementation can also find melodies distributed across multiple voices (polyphony). (Slides: &lt;a href=&quot;http://localhost:4000/assets/files/garfinkle-simssaxvii.pptx&quot;&gt;Powerpoint&lt;/a&gt; and &lt;a href=&quot;http://localhost:4000/assets/files/garfinkle-simssaxvii.pdf&quot;&gt;PDF&lt;/a&gt;)
&lt;img src=&quot;http://localhost:4000/assets/img/garfinkle-simssaxvii.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://music.columbia.edu/bios/eamonn-bell&quot;&gt;Eamonn Bell&lt;/a&gt; is a PhD student at Columbia and came to the workshop as a visiting guest. He shared his work gamifying crowdsourced machine learning ground truth development (Slides: &lt;a href=&quot;http://localhost:4000/assets/files/bell-simssaxvii.pdf&quot;&gt;PDF&lt;/a&gt;)
&lt;img src=&quot;http://localhost:4000/assets/img/bell-simssaxvii.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Julie Cumming presented her work with Cory McKay, using &lt;a href=&quot;http://jmir.sourceforge.net/jSymbolic.html&quot;&gt;jSymbolic&lt;/a&gt; to explore questions and assumptions about the origins of the madrigal. This included ways in which jSymbolic both confirmed and challenged Julie’s predictions, and leads us to a lot of interesting new things to explore. (Slides: &lt;a href=&quot;http://localhost:4000/assets/files/cumming-mckay-simssaxvii.pptx&quot;&gt;Powerpoint&lt;/a&gt; and &lt;a href=&quot;http://localhost:4000/assets/files/cumming-mckay-simssaxvii.pdf&quot;&gt;PDF&lt;/a&gt;)
&lt;img src=&quot;http://localhost:4000/assets/img/cumming-simssaxvii.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Audrey Laplante gave a presentation on the social barriers researchers face to doing digital scholarship. What sorts of rewards or consequences await those who are doing things differently? Computer use is sometimes still seen as a shortcut, or as shallower engagement than traditional scholarship. (Slides: &lt;a href=&quot;https://www.slideshare.net/audreylaplante1/social-barriers-to-digital-scholarship-for-arts-and-humanities-researchers&quot;&gt;Slideshare&lt;/a&gt;)
&lt;img src=&quot;http://localhost:4000/assets/img/laplante-simssaxvii.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Next up, we were delighted to have special guest &lt;a href=&quot;https://uniweb.uottawa.ca/members/2879&quot;&gt;Jada Watson&lt;/a&gt; come give a talk about her research. She is a Professor at the University of Ottawa, and shared her research using discographic metadata to study identity in country music. She introduced us to her &lt;a href=&quot;https://songdata.ca/&quot;&gt;SongData&lt;/a&gt; project, studying genres and networks through data. (Slides: &lt;a href=&quot;http://localhost:4000/assets/files/watson-simssaxvii.pptx&quot;&gt;Powerpoint&lt;/a&gt; and &lt;a href=&quot;http://localhost:4000/assets/files/watson-simssaxvii.pdf&quot;&gt;PDF&lt;/a&gt;)
&lt;img src=&quot;http://localhost:4000/assets/img/watson-simssaxvii.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The final set of presentations concerned the new SIMSSA Database. A successor of the ELVIS database, it’s designed to make symbolic files accessible and reusable. First I (Emily Hopkins) gave an introduction of our data model. This was originally developed by Cory McKay and presented at DLfM (&lt;a href=&quot;https://simssa.ca/publications&quot;&gt;McKay et al, 2017&lt;/a&gt;). This included an comparison with the &lt;a href=&quot;https://www.ifla.org/publications/node/11412&quot;&gt;IFLA-LRM&lt;/a&gt; in terms of describing a “musical work”, as well as how we track provenance and out feature-based content search. (Slides: &lt;a href=&quot;http://localhost:4000/assets/files/hopkins2-simssaxvii.pptx&quot;&gt;Powerpoint&lt;/a&gt; and &lt;a href=&quot;http://localhost:4000/assets/files/hopkins2-simssaxvii.pdf&quot;&gt;PDF&lt;/a&gt;)
&lt;img src=&quot;http://localhost:4000/assets/img/hopkins-simssaxvii.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Next up, Gustavo Polins Pedro and Yaolong Ju introduced the actual interface for the database. This included our search and upload functions, as well as the ways we use Wikidata, VIAF, and various controlled vocabularies to try to improve the quality and interoperability of our content. (Slides: &lt;a href=&quot;http://localhost:4000/assets/files/ju-polins_pedro-simssaxvii.key&quot;&gt;Keynote&lt;/a&gt; and &lt;a href=&quot;http://localhost:4000/assets/files/ju-polins_pedro-simssaxvii.pdf&quot;&gt;PDF&lt;/a&gt;)
&lt;img src=&quot;http://localhost:4000/assets/img/polins_pedro-simssaxvii.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;http://localhost:4000/assets/img/ju-simssaxvii.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Finally, Ichiro gave a big-picture presentation about building a larger infrastructure for music discovery, connecting all the different sources available out there and making them queryable. (Slides: &lt;a href=&quot;http://localhost:4000/assets/files/fujinaga2-simssaxvii.pdf&quot;&gt;PDF&lt;/a&gt;). We then ended the day with a group discussion.
&lt;img src=&quot;http://localhost:4000/assets/img/fujinaga-simssaxvii.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Thanks so much to everyone who presented, and special thanks to Vi-an Tran, Martha Thomae, Néstor Nápoles, and Yaolong Ju for their help with set-up, lunch, photos, note-taking, and more.&lt;/p&gt;

&lt;p&gt;Happy Holidays to those celebrating, and see you in 2019!&lt;/p&gt;</content><author><name>ehopkins</name></author><summary type="html"></summary></entry><entry><title type="html">Visiting Paris with Becky Shaw: DLfM 2018</title><link href="http://localhost:4000/blog/DLfM-becky-shaw/" rel="alternate" type="text/html" title="Visiting Paris with Becky Shaw: DLfM 2018" /><published>2018-10-31T00:00:00-04:00</published><updated>2018-10-31T00:00:00-04:00</updated><id>http://localhost:4000/blog/DLfM-becky-shaw</id><content type="html" xml:base="http://localhost:4000/blog/DLfM-becky-shaw/">&lt;p&gt;&lt;em&gt;The International Society for Music Information Retrieval conference (&lt;a href=&quot;http://ismir2018.ircam.fr/&quot;&gt;ISMIR 2018&lt;/a&gt;) was held in Paris this year, and several SIMSSA researchers shared their work at both ISMIR and the satellite conferences. These include SIMSSA Collaborator Jorge Calvo-Zaragoza’s brand-new &lt;a href=&quot;https://sites.google.com/view/worms2018/people&quot;&gt;WoRMS&lt;/a&gt; conference (Workshop on Reading Music Systems) as well as &lt;a href=&quot;https://dlfm.web.ox.ac.uk/&quot;&gt;DLfM&lt;/a&gt;, the International Conference on Digital Libraries for Musicology.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Becky Shaw has been a SIMSSA and Cantus Ultimus participant since 2016, working at SIMSSA Partner Dalhousie University, pursuing musicology and library science education while learning more about chant and digital tools for musicology.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This September, Becky went to &lt;a href=&quot;https://dlfm.web.ox.ac.uk/&quot;&gt;DLfM&lt;/a&gt; to share her research, presenting on &lt;a href=&quot;http://localhost:4000/assets/files/DLfM\_Presentation\_Shaw.pdf&quot;&gt;Differentiae in the Cantus Manuscript Database: Standardization and Musicological Application&lt;/a&gt;. Her work focuses on psalm differentia, the different endings to a psalm that can be used to make a smooth transition to the following antiphon. I got to ask her some questions about her trip to Paris and her research.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/becky_shaw_goatfell.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;Becky Shaw at Goat Fell, the highest point on the Isle of Arran in Scotland.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Emily Hopkins:&lt;/strong&gt; Had you visited Paris before going for DLfM? Was there in particular you were looking forward to on this trip?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Becky Shaw:&lt;/strong&gt; This was my first time in Paris, and—apart from the conference—I was really looking forward to visiting the &lt;a href=&quot;http://www.notredamedeparis.fr/en/&quot;&gt;Notre Dame Cathedral&lt;/a&gt; and the &lt;a href=&quot;http://www.sainte-chapelle.fr/en/&quot;&gt;Sainte Chappelle&lt;/a&gt;. It was a short trip, but I managed to see both, as well as visiting the &lt;a href=&quot;https://www.louvre.fr/en&quot;&gt;Louvre&lt;/a&gt; and &lt;a href=&quot;http://www.musee-moyenage.fr/&quot;&gt;Musée Cluny&lt;/a&gt;, where I particularly enjoyed the exhibit on the &lt;a href=&quot;http://www.musee-moyenage.fr/collection/oeuvre/la-dame-a-la-licorne.html&quot;&gt;Lady and the Unicorn tapestries&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/becky_shaw_notre-dame.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;Visiting Notre Dame.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;EH:&lt;/strong&gt; How did you develop the research question that led to your DLfM paper? Give us a brief overview of the research that you presented. &lt;em&gt;(You can view the slides &lt;a href=&quot;http://localhost:4000/assets/files/DLfM_Presentation_Shaw.pdf&quot;&gt;here&lt;/a&gt;).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;BS:&lt;/strong&gt; The research that I presented at &lt;a href=&quot;https://dlfm.web.ox.ac.uk/&quot;&gt;DLfM&lt;/a&gt; was related to work that I have done over the past two years as a Research Assistant with the &lt;a href=&quot;http://cantus.uwaterloo.ca/&quot;&gt;Cantus Manuscript Database&lt;/a&gt;, standardizing the differentiae field for the manuscript indices. I also presented some of my preliminary work on the relationship between differentiae and the melodic incipits of antiphons, which will be a part of my master’s thesis that I am currently working on. Prior to my work on the standardization of the indexing of differentiae, large-scale computer analyses of the function of these melodic formulas was not possible, and much can be garnered from such analyses about how differentiae were used in medieval chant. The study of such a standardized element between manuscripts can also help in studies of chant transmission and other geo-temporal relationships.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/becky_shaw_differentia-examples.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;EH:&lt;/strong&gt; What sorts of technical skills are required in this research, and how did you come to develop them?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;BS:&lt;/strong&gt; Most of the analysis that I completed for this paper was done using Excel, as were the first stages of the differentiae standardization project for Cantus. I am currently working on an interactive online database that can be used to implement the differentiae standardization system in future manuscript indices and for the further study of differentiae using an open-sourced and widely-used content management system called &lt;a href=&quot;https://www.drupal.org/&quot;&gt;Drupal&lt;/a&gt;. I have also done further analysis of the differentiae data using &lt;a href=&quot;https://public.tableau.com/en-us/s/&quot;&gt;Tableau Public&lt;/a&gt;. I developed most of the basic skills through my Master of Library and Information Studies’ courses at Dalhousie University, and taught myself additional skills as necessary to adapt these tools for my specific purposes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/becky_shaw_differentia-chart.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;EH:&lt;/strong&gt; What were some highlights of other research you encountered at DLfM?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;BS:&lt;/strong&gt; There was a wide range of other topics presented at DLfM, from the creation of tools to genre-specific corpuses, advancements in and uses of OMR and &lt;a href=&quot;https://music-encoding.org/&quot;&gt;MEI&lt;/a&gt;, and more. It was a packed day,and I certainly learned a lot! I was particularly intrigued by a presentation from David Lewis, David M. Weigl, Joanna Bullivant, and Kevin Page on “Publishing musicology using multimedia digital libraries,” and the possibilities that their framework tool, &lt;a href=&quot;http://www.oerc.ox.ac.uk/news/centre-releases-music-encoding-software&quot;&gt;Music Encoding and Linked Data (MELD)&lt;/a&gt;, enables for both academic and public musicology digital publications, combining various media sources to enhance the user’s experience.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;EH:&lt;/strong&gt; You recently did an internship with the &lt;a href=&quot;http://www.bnportugal.pt/&quot;&gt;Biblioteca Nactional de Portugal (BNP)&lt;/a&gt; and &lt;a href=&quot;http://cesem.fcsh.unl.pt/&quot;&gt;CESEM&lt;/a&gt; (where SIMSSA collaborator Elsa de Luca also works!) Can you tell us a little bit about your work there?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;BS:&lt;/strong&gt; The summer before last (2017), I spent 6 weeks in Lisbon, where I completed my practicum placement for my MLIS degree, which was made possible through Kate Helsen, who taught me during my undergraduate degree at Western University. While there, I split my time between the BNP and CESEM. At the BNP, I worked with Sílvia Sequeira, head of the music division, to catalogue a collection of chant books donated to the library in the 1990s by the National Conservatory of Lisbon, which were previously held at various monasteries in Portugal. At CESEM, I worked in a small library that services research centres in the Faculty of Social Sciences and Humanities, ID–Research and PhDs Documentation Centre, to start adding their score collection to their online catalogue.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;EH:&lt;/strong&gt; What’s up next for you, particularly in terms of directions for your research and schooling in both music and library science?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;BS:&lt;/strong&gt; I recently completed my Master of Library and Information Studies (MLIS) at Dalhousie (graduated May 2018), and am in the final year of my Master of Arts in musicology, studying with Jennifer Bain. This year I am finishing up the required course work and writing my thesis, which I am aiming to complete this summer for graduation October 2019. I also continue to work as a Research Assistant for Cantus, and am a student intern at the Dalhousie University Archives. Following the completion of my studies at Dalhousie, I will be pursuing a career in music librarianship and/or archives, and hope to continue following my music research interests in plainchant and digital musicology. At some point, I may also pursue a doctorate in musicology.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/becky_shaw_seine.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;A parting shot of the Seine.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thanks Becky for taking the time to share with us! We’re looking forward to ISMIR 2020, which we will host here in Montreal!&lt;/em&gt;&lt;/p&gt;</content><author><name>ehopkins</name></author><summary type="html">The International Society for Music Information Retrieval conference (ISMIR 2018) was held in Paris this year, and several SIMSSA researchers shared their work at both ISMIR and the satellite conferences. These include SIMSSA Collaborator Jorge Calvo-Zaragoza’s brand-new WoRMS conference (Workshop on Reading Music Systems) as well as DLfM, the International Conference on Digital Libraries for Musicology.</summary></entry><entry><title type="html">A Trip to Leipzig: SIMSSA XV and IAML</title><link href="http://localhost:4000/blog/simssa-xv-in-leipzig/" rel="alternate" type="text/html" title="A Trip to Leipzig: SIMSSA XV and IAML" /><published>2018-08-30T00:00:00-04:00</published><updated>2018-08-30T00:00:00-04:00</updated><id>http://localhost:4000/blog/simssa-xv-in-leipzig</id><content type="html" xml:base="http://localhost:4000/blog/simssa-xv-in-leipzig/">&lt;p&gt;Our fifteenth SIMSSA Workshop took place in Leipzig on 28 July 2018 immediately following the annual congress of the &lt;a href=&quot;http://iaml2018.info/home/&quot;&gt;International Association of Music Librarians&lt;/a&gt;! Many of our partners and collaborators work in music libraries so it was a great chance to meet some of them and update each other on our work. In addition to my work for SIMSSA, I am also doing my MLIS, so it was really wonderful to get to meet so many people working on different aspects of librarianship and music. It was also fascinating to get a chance to learn more about Leipzig’s history as a centre for classical music and also as part of East Germany.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/nikolaikirche.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;&lt;em&gt;The Nikolaikirche downtown. Bach worked here as well as the St. Thomas church, and it was also the site of the Monday demonstrations, peaceful protests against the German Democratic Republic from 1989-1991.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;The IAML Congress started on 22 July and continued all week, with SIMSSA XV taking place on Saturday. There was also some time to explore Leipzig (see image below of a mural in Plazwig.) There were too many interesting talks to cover everything, but I’ll share some personal highlights below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/plazwig.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I got to attend a &lt;a href=&quot;http://www.rism.info/home/&quot;&gt;RISM&lt;/a&gt; training session led by Jennifer Ward on their online music cataloging software, &lt;a href=&quot;http://www.rism.info/community/muscat/&quot;&gt;Muscat&lt;/a&gt;. This was especially interesting to me for our work in the lab designing the new and improved SIMSSA Database this summer (more on that later…). One of the most interesting features is how Muscat integrates &lt;a href=&quot;http://viaf.org/&quot;&gt;VIAF&lt;/a&gt; data into its cataloging practises. &lt;a href=&quot;http://viaf.org/&quot;&gt;VIAF&lt;/a&gt; is the Virtual International Authority File, uniting the collections and cataloging processes of libraries around the world with unique identifiers, making it possible to have linked data approaches to music databases.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.rism.info/community/muscat/&quot;&gt;&lt;img src=&quot;http://localhost:4000/assets/img/muscat-logo.png&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://library.usask.ca/people/carolyn-doi.php&quot;&gt;Carolyn Doi&lt;/a&gt; presented on research she’s doing with &lt;a href=&quot;https://www.library.ualberta.ca/staff/sean-luyk&quot;&gt;Sean Luyk&lt;/a&gt; through a SSHRC-funded project called &lt;em&gt;Sounds of home: local music collecting and collections in Canada&lt;/em&gt;. In the presentation, Carolyn shared some results from the midway point of their project, highlighting survey responses from libraries with local music collections. Their project includes documenting and describing local music collections, exploring who uses them and for what purposes, and ways to support local collections. This includes asking questions about respectful ways to engage with communities and music from traditionally marginalized groups and underrepresented communities. How can music be preserved in a respectful and inclusive way?&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.lynnseyweissenberger.com/&quot;&gt;Lynnsey Weissenberger&lt;/a&gt; shared her work on LITMUS, the Linked Irish Traditional Music Database, a project of the &lt;a href=&quot;https://www.itma.ie/&quot;&gt;Irish Traditional Music Archive&lt;/a&gt;. This two-year project involves the development of a traditional music database and an appropriate linked data ontology for Irish traditional music to go with it. Linked data triples allow for the description of relationships between tunes or other source materials – for examples, stories or poems associated with a tune, or which collections it can be found in. This database also makes use of &lt;a href=&quot;https://loc.gov/item/2015655578&quot;&gt;Traditional Knowledge labels&lt;/a&gt; (Here is an example of these labels in context in a Library of Congress entry: &lt;a href=&quot;https://loc.gov/item/2015655578&quot;&gt;Passamaquoddy War song; Trading song&lt;/a&gt;). There is also great potential for the work done on LITMUS to be used for databases of other traditional music in the future.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://litmus.itma.ie/&quot;&gt;&lt;img src=&quot;http://localhost:4000/assets/img/litmus-logo.png&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Cecile Cecconi presented on &lt;em&gt;Music and Linked Open Data: Results and Lessons Learned from the DOREMUS project.&lt;/em&gt; &lt;a href=&quot;http://www.doremus.org/&quot;&gt;Doremus&lt;/a&gt; is short for Doing Reusable Musical Data. Their work includes an ontology for linked data, multilingual controlled vocabularies, and different approaches to music metadata. The Doremus ontology is available on GitHub &lt;a href=&quot;https://github.com/DOREMUS-ANR/doremus-ontology&quot;&gt;here&lt;/a&gt;. Cecconi also discussed &lt;a href=&quot;http://yamplusplus.lirmm.fr/&quot;&gt;Yam++&lt;/a&gt;, a web tool for ontology and thesaurus matching, and even demonstrated a &lt;a href=&quot;https://github.com/D2KLab/music-chatbot&quot;&gt;Doremus chatbot&lt;/a&gt; that uses natural language processing to try to answer music questions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.doremus.org/&quot;&gt;&lt;img src=&quot;http://localhost:4000/assets/img/doremus-logo.png&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;After the main congress was over, SIMSSA XV took place on Saturday, in the University Library at the Hochschule für Musik und Theater.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/simssaxv.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;First up, Ichiro Fujinaga gave a brief welcome and introduction to the workshop. Next, I (Emily Hopkins) presented an overview of the SIMSSA project, including a technical explanation of our goals as well as an update on current research projects, including Karen’ Desmond’s &lt;a href=&quot;http://measuringpolyphony.org/&quot;&gt;Measuring Polyphony Project&lt;/a&gt;. Slides are available &lt;a href=&quot;http://localhost:4000/assets/files/hopkins_simssaxv.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Jürgen Diet gave a talk on &lt;a href=&quot;http://localhost:4000/assets/files/diet_simssaxv.pdf&quot;&gt;The OMR Project of the Bavarian State Library&lt;/a&gt;. He gave an overview of their OMR project, including hiring Sanu Pulimootil to work on the project. They tested OMR software such as Audiveris, Capella Scan, SharpEye, and SmartScore. From this, they developed a set of OMR results using the BSB’s digitized score collection, and then built a search interface that uses a keyboard for musical queries (see below).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/bsb_search.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Cory McKay introduced &lt;a href=&quot;http://localhost:4000/assets/files/mckay_simssaxv.pdf&quot;&gt;SIMSSA DB: A Database for Computational Musicological Research&lt;/a&gt;.
This database is for storing and organizing symbolic music files and research studies, dealing with the best ways of handling metadata for a wide range of musical materials. His presentation gave an overview of the considerations that went into the new data model as well as the progress we’ve made this summer. Here is the entity-relationship diagram for the database:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/simssadb_erd.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Next, Laurent Pugin gave us an update on the latest developments in &lt;a href=&quot;https://www.verovio.org/index.xhtml&quot;&gt;Verovio&lt;/a&gt;, including harmonic annotations, better handling of trill extensions and text that extends across multiple barlines, and PDF score output. You can see his slides &lt;a href=&quot;http://localhost:4000/assets/files/pugin_simssaxv.pdf&quot;&gt;here&lt;/a&gt;. He also showed a neat example of how you can separate the layers of different kinds of symbols. Here they are highlighted:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/verovio1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And then separated into layers like the kind we use for our OMR process:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/verovio2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Finally, Ichiro Fujinaga gave a detailed overview of how the SIMSSA OMR process works, showcasing the progress our summer students have made in the lab this year at McGill.&lt;/p&gt;

&lt;p&gt;After the workshop, several workshop participants went out for lunch at a nearby restaurant.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/simssaxv_lunch.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;&lt;em&gt;*L-R:* Emily Hopkins (McGill), Sonia Wronkowska (National Library of Poland), André Avorio (Alexander Street Press), Laurent Pugin (RISM-CH), Jane Gottlieb (The Juilliard School), Ichiro Fujinaga (McGill), Barbara Dobbs Mackenzie (RILM), Craig Sapp (Stanford University), Cory McKay (Marianapolis College), Erin Conor (University of Washington, and Tim Crawford (Goldsmiths, University of London)).&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;It was a real pleasure to attend my first IAML and visit Leipzig. Thank you to everyone I met and also who attended the SIMSSA Workshop – be sure to let us know if you’re ever in Montreal!&lt;/p&gt;

&lt;p&gt;I will leave you with a final picture of the smallest Montreal IAML delegate paying her respects to Leipzig’s most famous resident.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/bach_baby.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;</content><author><name>ehopkins</name></author><summary type="html">Our fifteenth SIMSSA Workshop took place in Leipzig on 28 July 2018 immediately following the annual congress of the International Association of Music Librarians! Many of our partners and collaborators work in music libraries so it was a great chance to meet some of them and update each other on our work. In addition to my work for SIMSSA, I am also doing my MLIS, so it was really wonderful to get to meet so many people working on different aspects of librarianship and music. It was also fascinating to get a chance to learn more about Leipzig’s history as a centre for classical music and also as part of East Germany.</summary></entry><entry><title type="html">SIMSSA XIV at McGill</title><link href="http://localhost:4000/blog/Workshop-on-SIMSSA-XIV/" rel="alternate" type="text/html" title="SIMSSA XIV at McGill" /><published>2018-06-25T00:00:00-04:00</published><updated>2018-06-25T00:00:00-04:00</updated><id>http://localhost:4000/blog/Workshop-on-SIMSSA-XIV</id><content type="html" xml:base="http://localhost:4000/blog/Workshop-on-SIMSSA-XIV/">&lt;p&gt;At the end of May, we were pleased to host our fourteenth SIMSSA Workshop! Held at McGill University, our guest speakers included Andy Irving (&lt;a href=&quot;https://www.bl.uk/projects/save-our-sounds&quot;&gt;British Library&lt;/a&gt;), Laurent Pugin (&lt;a href=&quot;http://rism-ch.org/&quot;&gt;RISM-Switzerland&lt;/a&gt;), Jorge Calvo-Zaragoza (&lt;a href=&quot;https://www.upv.es/index-en.html&quot;&gt;Universitat Politècnica de Valencia&lt;/a&gt;, and Andrew Hankinson (&lt;a href=&quot;https://www.bodleian.ox.ac.uk/&quot;&gt;Bodleian Libraries at Oxford&lt;/a&gt;.) We also heard presentations from our summer students, who are developing SIMSSA software full-time in the DDMAL lab from May through August.&lt;/p&gt;

&lt;p&gt;First up, we had Andy Irving present on &lt;a href=&quot;http://localhost:4000/assets/files/UOSH-IIIF-AV-BL_May2018.pdf&quot;&gt;Unlocking our sound heritage: IIIF AV at the BL&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/andy-irving.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://iiif.io/&quot;&gt;IIIF&lt;/a&gt; may be familiar to many SIMSSA folks at this point – it’s the International Image Interoperability Framework. This technology is what enables &lt;a href=&quot;https://musiclibs.net&quot;&gt;MusicLibs&lt;/a&gt;, where we serve searchable IIIF manifests of score images that are still hosted at their home institutions. &lt;a href=&quot;http://iiif.io/community/groups/av/&quot;&gt;IIIF AV&lt;/a&gt; involves extending this concept to audio and video materials, including creating a Universal Player. Andy presented on using IIIF AV at the British Library, highlighting the Save our Sounds project (More &lt;a href=&quot;https://www.bl.uk/projects/save-our-sounds&quot;&gt;here&lt;/a&gt;). You can also follow Andy on Twitter &lt;a href=&quot;https://twitter.com/ndyirving&quot;&gt;@ndyirving&lt;/a&gt; and also follow &lt;a href=&quot;https://twitter.com/BLSoundHeritage&quot;&gt;@BLSoundHeritage&lt;/a&gt; for more.&lt;/p&gt;

&lt;p&gt;Jorge Calvo-Zaragoza presented next on the challenge of developing better OMR technology. A former SIMSSA postdoc, he’s currently a postdoc at the Universitat Politècnica de Valencia. He presented &lt;a href=&quot;http://localhost:4000/assets/files/human-aided_OMR_May2018.pdf&quot;&gt;Towards human-aided document processing for Optical Music Recognition&lt;/a&gt;. His latest research is moving from a Pixelwise classification method using Convolutional Neural Nets (CNN) to a Patchwise classification method that uses auto-encoders to replace the CNN.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/jorge-calvo-zaragoza.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Jorge is also one of the chairs of the new &lt;a href=&quot;https://sites.google.com/view/worms2018/home?authuser=0&quot;&gt;Workshop on Reading Music Systems (WORMS)&lt;/a&gt;; the first edition will take place as a satellite event to &lt;a href=&quot;http://ismir2018.ircam.fr/&quot;&gt;ISMIR 2018&lt;/a&gt; in Paris. They are still &lt;a href=&quot;https://sites.google.com/view/worms2018/call-for-papers?authuser=0&quot;&gt;accepting paper submissions&lt;/a&gt; until 15 July! You can follow him on Twitter &lt;a href=&quot;https://twitter.com/jcalvozaragoza&quot;&gt;@jcalvozaragoza&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Andrew Hankinson has been with the SIMSSA Project since before there was a SIMSSA project, first as a PhD student at McGill and then as a Postdoc. He now works at the &lt;a href=&quot;https://www.bodleian.ox.ac.uk/&quot;&gt;Bodleian Libraries at Oxford&lt;/a&gt; and continues to contribute to SIMSSA as a Collaborator. He presented on &lt;a href=&quot;http://localhost:4000/assets/files/hankinson-digital-bodleian.pdf&quot;&gt;IIIF resource delivery at the Bodleian&lt;/a&gt;], describing his work towards the upcoming Digital Bodleian 2.0.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/andrew-hankinson.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This includes digitization workflows, curating data and metadata, long-term digital preservation strategies, as well as user interface and APIs for discovery. You can follow him on Twitter &lt;a href=&quot;https://twitter.com/ahankinson&quot;&gt;@ahankinson&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Our next presenter, Laurent Pugin, works with &lt;a href=&quot;http://rism-ch.org/&quot;&gt;RISM&lt;/a&gt; in Switzerland. He presented on “Music notation online editing” giving an overview of the different options for web-based music editors, including &lt;a href=&quot;https://noteflight.com&quot;&gt;noteflight.com&lt;/a&gt;, &lt;a href=&quot;https://flat.io&quot;&gt;flat.io&lt;/a&gt;, &lt;a href=&quot;https://wiki.de.dariah.eu/pages/viewpage.action?pageId=7439804&quot;&gt;Meise&lt;/a&gt;, and &lt;a href=&quot;https://atom.io/packages/mei-tools-atom&quot;&gt;Atom MEI tools&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/laurent-pugin.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Laurent is of course also the creator of &lt;a href=&quot;https://www.verovio.org/index.xhtml&quot;&gt;Verovio&lt;/a&gt;, which was also featured in the presentation (it plays a part in Atom’s MEI tools, as well as in Craig Sapp’s &lt;a href=&quot;https://verovio.humdrum.org/&quot;&gt;Verovio Humdrum Viewer&lt;/a&gt;, and is part of Neon – more on Neon below.) You can follow Laurent on Twitter &lt;a href=&quot;http://twitter.com/lxpugin&quot;&gt;@lxpugin&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The second half of the workshop featured our summer students. Every summer we hire several McGill students to work as full-time developers in our lab over the summer. They work alongside postdocs and graduate students in our lab, developing the different componenets of the optical music recognition (OMR), search, and analysis processes.&lt;/p&gt;

&lt;p&gt;First up, Eric Liu presented his work on &lt;a href=&quot;https://github.com/ddmal/pixel.js&quot;&gt;Pixel.js&lt;/a&gt;, originally developed last year by Ké Zhang and Zeyad Saleh.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/eric-liu.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Eric has been expanding on their work and integrating Pixel into the overall workflow, so he presented on work he’s done with &lt;a href=&quot;https://simssa.ca/blog/interviewing-martha]&quot;&gt;Martha Thomae&lt;/a&gt;, &lt;a href=&quot;http://localhost:4000/assets/files/Pixel_May2018.pdf&quot;&gt;Pixel.js and the Calvo Classifier as a Rodan Workflow&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Minh Anh Nguyen demonstrated her recent work on &lt;a href=&quot;http://localhost:4000/assets/files/IC_presentation_final.pptx&quot;&gt;Updates to RODAN Gamera Interactive Classifier&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/minh-anh-nguyen.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Recent work includes its integration into our workflow manager, Rodan, as well as testing from collaborators Inga Behrendt and Jennifer Bain. New functionality for correction has also been added, including deleting and renaming classes, importing and saving classes and subclasses, and deleting glyphs as needed.&lt;/p&gt;

&lt;p&gt;Noah Baxter is tackling the pitch finding part of the OMR process this summer.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/noah-baxter.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;He’s been working with a staff-finding algorithm to try to find centre-weighting for each note glyph and use that to determine pitch. Check out his slides for more: &lt;a href=&quot;http://localhost:4000/assets/files/pitch-finding_May2018.pdf&quot;&gt;Heuristic Pitch Finding&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Zoé McLennan and Andrew Tran both worked for us last summer, too, and we’ve added Juliette Regimbal to the Neon team this year. In their presentation,
&lt;a href=&quot;http://localhost:4000/assets/files/Neon2_May2018.pdf&quot;&gt;Neon2: Redesigning a web-based MEI neume editor&lt;/a&gt;, they talk about integrating Verovio into Neon2 (see Andrew Tran, below).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/andrew-tran.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The original version of Neon was developed before Verovio existed, and used &lt;a href=&quot;http://fabricjs.com/&quot;&gt;fabric.js&lt;/a&gt; as a rendering tool. Verovio is a much more efficient tool for rendering music as it was designed for that purpose. They also talked about plans for Neon3, which incorporates editing tools into the new framework built in Neon2 (see Juliette Regimbal, below).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/juliette-regimbal.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Finally, the SIMSSA Database is designed as a repository for symbolic music files. Originally conceived as the &lt;a href=&quot;https://database.elvisproject.ca/&quot;&gt;ELVIS Database&lt;/a&gt; as part of the ELVIS Digging into Data grant many years ago, this newest iteration is based on a new data model developed last year by Cory McKay, Andrew Hankinson, Julie Cumming, and Ichiro Fujinaga. In the lab Gustavo Polins Pedro and Andrew Kam have been working on putting this model into practise, as well as updating and improving the user interface and metadata. Below, Andrew Kam discusses querying a relational database for information about chants.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/andrew-kam.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here, Gustavo describes some experiments done with graph database technology for our project.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/gustavo-polins-pedro.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can see their slides at &lt;a href=&quot;http://localhost:4000/assets/files/SIMSSADB_May2018.pdf&quot;&gt;SIMSSA Database&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Thanks to everyone who presented!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/SIMSSAXIV-groupshot.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The next SIMSSA Workshop will be in Leipzig, after &lt;a href=&quot;http://iaml2018.info/home/&quot;&gt;IAML&lt;/a&gt;! Maybe we’ll see you there!&lt;/p&gt;</content><author><name>ehopkins</name></author><summary type="html">At the end of May, we were pleased to host our fourteenth SIMSSA Workshop! Held at McGill University, our guest speakers included Andy Irving (British Library), Laurent Pugin (RISM-Switzerland), Jorge Calvo-Zaragoza (Universitat Politècnica de Valencia, and Andrew Hankinson (Bodleian Libraries at Oxford.) We also heard presentations from our summer students, who are developing SIMSSA software full-time in the DDMAL lab from May through August.</summary></entry><entry><title type="html">Workshop on Digital Musicology</title><link href="http://localhost:4000/blog/Workshop-on-digital-musicology/" rel="alternate" type="text/html" title="Workshop on Digital Musicology" /><published>2018-05-04T00:00:00-04:00</published><updated>2018-05-04T00:00:00-04:00</updated><id>http://localhost:4000/blog/Workshop-on-digital-musicology</id><content type="html" xml:base="http://localhost:4000/blog/Workshop-on-digital-musicology/">&lt;p&gt;On 27 April, we held a workshop with &lt;a href=&quot;https://www.cirmmt.org/&quot;&gt;CIRMMT&lt;/a&gt; RA 2 – Music Information Research. This workshop was designed as a companion to Eleanor Selfridge-Field’s CIRMMT Distinguished Lecture. Unfortunately her visit to Montreal had to be cancelled but we were able to hold the workshop anyways, and I’ve included all the slides here for anyone who was not able to attend but wants to know more!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/group-shot-27april.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The first presentation was from Claire Arthur, based on work done towards a chapter in the forthcoming &lt;em&gt;Oxford Handbook of Music and Corpus Studies&lt;/em&gt; with Peter Schubert and Julie Cumming. The presentation was titled &lt;a href=&quot;http://localhost:4000/assets/files/claire-arthur-april27.pptx&quot;&gt;“Whose Line is it Anyway?: Assessing Melodic Features of Mode in Polyphony.”&lt;/a&gt; For this presentation, Claire described their study’s investigation of the nature of mode in polyphonic music. Using 44 Renaissance duos with modal labels taken from treatises and didactic collections, they examined which features were best for computationally determining mode, comparing the computer’s guesses to those from early music experts.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/claire-april27.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Next up, Martha Thomae presented on the &lt;a href=&quot;http://localhost:4000/assets/files/martha-thomae-april27.pptx&quot;&gt;“Automatic Scoring-Up of Mensural Parts.”&lt;/a&gt; Before the development of modern common Western music notation, there was mensural music. Starting in the 13th century, this style had different shapes and colours of noteheads to indicate rhythm, but there were no barlines, and some of the interpretation depends on context and mensuration signs (an early ancestor of the time signature.) This music was distributed in handwritten partbooks (shown below behind Martha), where each voice was in a separate part of the page or book (in contrast with a more modern score where the parts are aligned.) This can make analysis and performance challenging.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/martha-april27.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Martha devised this algorithmic implementation of 13th-century music theory rules to allow early music part books to be automatically aligned. This will have applications for counterpoint study and performance, but also as part of the larger OMR process we are building with SIMSSA. Check out our &lt;a href=&quot;https://simssa.ca/blog/interviewing-martha&quot;&gt;previous interview&lt;/a&gt; with her for more discussion of this research.&lt;/p&gt;

&lt;p&gt;The next paper focussed on &lt;a href=&quot;http://www.music.mcgill.ca/~cmckay/papers/musictech/mckay18corpus.pdf&quot;&gt;Methodologies for Creating Symbolic Early Music Corpora for Musicological Research.&lt;/a&gt; This paper came out of Julie Cumming’s research on madrigals. She wanted to build a corpus and use Cory MacKay’s jSymbolic software to do machine learning, looking at questions about the origin of the genre. Julie was away at a &lt;a href=&quot;http://www.troja-online.eu/kolloquium-2018/&quot;&gt;digital humanities conference in Mainz&lt;/a&gt; (presenting “Why Should Musicologists do Digital Humanities?”), so Cory presented on her behalf. Other co-authors Jonathan Stuchbery, an undergradute in music performance who was very involved in the editing and transcription process, and SIMSSA Project Director Ichiro Fujinaga.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/cory-methodology-april27.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When creating a corpus of symbolic music files, different notation programs and settings can effect the end results, potentially making them artificially good or adding unecessary noise. In this study, they investigated the details of the transcription process, going from original score to symbolic file. We don’t want to run machine learning experiments where we get 100% results when the real difference we are detecting is that between Sibelius and Finale! The paper talks about different areas of concern and suggests best practises for the future.&lt;/p&gt;

&lt;p&gt;Finishing the first half of the workshop, Ian Lorenz presented on work he did at Citations: the Renaissance Imitation Mass, which held summers of analysis in 2016 and 2017. Work done at CRIM included the development of a Controlled Vocabulary for discussing different musical presentation types in the Renaissance imitation mass, as well as the development of EMA citations (Enhancing Music Notation Accessibility.) This method uses &lt;a href=&quot;http://www.verovio.org/index.xhtml&quot;&gt;Verovio&lt;/a&gt; to render &lt;a href=&quot;http://music-encoding.org/&quot;&gt;MEI&lt;/a&gt; in the browser, and then allows the researcher to select a part of the score and create a stable URL that links back to the score. Each example of a presentation type can be linked to directly in the score.
In &lt;a href=&quot;http://localhost:4000/assets/files/ian-lorenz-april27.pdf&quot;&gt;Cadéac, Gobert, and CRIM: A New Approach to the Renaissance Imitation Mass&lt;/a&gt;, Ian showed how he can use tools from CRIM to gain new insights into the work of Gombert. For more, see our blog post interviewing him about the project &lt;a href=&quot;https://simssa.ca/blog/interview-with-ian-lorenz&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/ian-april27.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;After the coffee break, Cory McKay gave an overview and tutorial of his software jSymbolic; you can view the slides on his site &lt;a href=&quot;http://www.music.mcgill.ca/~cmckay/papers/musictech/mckay18demonstration.pdf&quot;&gt;here&lt;/a&gt;. jSymbolic extracts features from MIDI files, allowing us to do machine learning experiments in a wide range of genres. These include some impressive results getting jSymbolic to guess the composer within a repertory. Cory demonstrated some ways to visualize features, like this histogram comparing vertical 6ths in Ockeghem versus Josquin.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/ockeghem-josquin.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;jSymbolic also works with &lt;a href=&quot;https://www.cs.waikato.ac.nz/ml/weka/&quot;&gt;Weka&lt;/a&gt; data mining software. You can learn more about jSymbolic on Cory’s site &lt;a href=&quot;http://jmir.sourceforge.net/index_jSymbolic.html&quot;&gt;here&lt;/a&gt; and you can also check out in our blog post annnouncing the release of jSymbolic2 &lt;a href=&quot;https://simssa.ca/blog/jsymbolic2-released&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Before coming to McGill, Néstor Nápoles studied at the &lt;a href=&quot;https://www.upf.edu/web/mtg&quot;&gt;Universitat Pompeu Fabra&lt;/a&gt;. For this workshop, he presented on the work he did for his Maste’rs thesis with the &lt;a href=&quot;http://compmusic.upf.edu/&quot;&gt;CompMusic&lt;/a&gt; project, doing automatic harmonic analysis for string quartets: &lt;a href=&quot;http://localhost:4000/assets/files/nestor-napoles-april27.pdf&quot;&gt;Dataset Creation and Automatic Harmonic Analysis for Six Quartets from Op. 20 by Joseph Haydn&lt;/a&gt; He created a dataset of Haydn’s Op. 20 using &lt;a href=&quot;http://www.humdrum.org/&quot;&gt;Humdrum&lt;/a&gt;, using the &lt;a href=&quot;http://www.humdrum.org/rep/harm/&quot;&gt;**harm&lt;/a&gt; syntax to encode human-annotated functional harmony. He then compared these results to automatic annotation done with existing algorithms (Temperley, 1997; Temperley &amp;amp; Sleator, 2001; Sapp, 2013).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/nestor-april27.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Finally, Nat Condit-Schultz introduced his project &lt;a href=&quot;http://localhost:4000/assets/files/nat-condit-schultz-april27.svg&quot;&gt;“Elision and Enjambment in Musical Lyrics: A Systematic Analysis of Text-Music Organization in Art and Popular Song.”&lt;/a&gt; (zoom in to see the details.) Nat’s research looks at musical phrases and text phrases and the ways in which they align – and don’t align. He emphasized the important of including text when we encode music.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/nat-april27.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A brief round-table discussion followed, including looking at better ways to connect developers and musicologists. At SIMSSA we connect these groups on a regular basis, in our weekly meetings and in workshops like this one, but it can still be hard to bridge the gap between the sorts of questions people want to ask, the tools we can build, and questions we haven’t even considered asking yet. Music also poses unique challenges for study through machine learning – when even the expert humans can’t agree about the mode of a given piece, what can we expect from the computer? Turning implicit rules of music theory into explicit algorithms often reveals gaps in the theory, or reveals just how much is still ambiguous or subjective.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/table-discussion.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Thank you to CIRMMT and everyone who presented their research and helped run the workshop!&lt;/p&gt;</content><author><name>ehopkins</name></author><summary type="html">On 27 April, we held a workshop with CIRMMT RA 2 – Music Information Research. This workshop was designed as a companion to Eleanor Selfridge-Field’s CIRMMT Distinguished Lecture. Unfortunately her visit to Montreal had to be cancelled but we were able to hold the workshop anyways, and I’ve included all the slides here for anyone who was not able to attend but wants to know more!</summary></entry><entry><title type="html">An Interview with Johanna Devaney</title><link href="http://localhost:4000/blog/johanna-devaney/" rel="alternate" type="text/html" title="An Interview with Johanna Devaney" /><published>2018-04-12T00:00:00-04:00</published><updated>2018-04-12T00:00:00-04:00</updated><id>http://localhost:4000/blog/johanna-devaney</id><content type="html" xml:base="http://localhost:4000/blog/johanna-devaney/">&lt;p&gt;&lt;em&gt;&lt;a href=&quot;http://www.devaney.ca&quot;&gt;Johanna Devaney&lt;/a&gt; is Assistant Professor of Music Theory and Cognition at &lt;a href=&quot;https://music.osu.edu/people/devaney.12&quot;&gt;Ohio State University&lt;/a&gt;, currently teaching in the &lt;a href=&quot;https://steinhardt.nyu.edu/music/technology&quot;&gt;music technology program at NYU&lt;/a&gt;, and the newest SIMSSA Collaborator. We had a chance to talk over Skype about music and the digital humanities, learning to program, and the ways that music research can drive technological advances even beyond disciplinary bounds.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/devaney_sm.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;EH:&lt;/strong&gt; Thank you so much for agreeing to talk with me today. We’re pretty excited to have you on board as a SIMSSA collaborator. Since music information retrieval as I’ve experienced it attracts folks with a lot of different disciplinary interests, I’m always curious about the paths people have taken to end up here. I looked at your CV and it looks like you started out playing piano and oboe and doing some composing, and then you did a BFA in music and history, and then a computer programming diploma, all before ever getting into Music Information Retrieval specifically. Can you tell me a little bit more about that early education and how you got into computer programming?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;JD:&lt;/strong&gt; I’d always been interested in using computers. I’m old enough that when I got my first computer the way that you got new games was either coding them or downloading them from bulletin board systems. I’d always found computers really interesting but I didn’t actually pursue that through my undergraduate. When I graduated with my BFA, I wanted to learn more about programming, so I decided to do the computer programming diploma. A year into the diploma, I started a Master’s at York in Composition, and worked on both simultaneously for a year. Initially, I was focused on applying programming in my compositions. This expanded to research due to interest in in my composing for vocalists and unfretted string instruments capable of flexible intonation. I wanted my performance instructions to extend what they were doing, as opposed to just prescribe something completely different. During the course of my Master’s, I realized that there was very little written about this, so I thought, well, this would be an interesting subject for a PhD. This ultimately led me to &lt;a href=&quot;http://ddmal.music.mcgill.ca/&quot;&gt;Ich’s lab&lt;/a&gt; at McGill.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;EH:&lt;/strong&gt; How did your time at McGill influence work you’ve done since then, going on in MIR?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;JD:&lt;/strong&gt; The great thing about the &lt;a href=&quot;http://www.mcgill.ca/music/home&quot;&gt;Schulich School of Music&lt;/a&gt; at McGill is opportunity to engage with both the technical and the musicological. In addition to working with Ich, I worked closely with &lt;a href=&quot;http://www.mcgill.ca/music/jonathan-wild&quot;&gt;Jon Wild&lt;/a&gt; and &lt;a href=&quot;http://www.mcgill.ca/music/peter-schubert&quot;&gt;Peter Schubert&lt;/a&gt; on all of the singing voice work that I was doing. Also &lt;a href=&quot;https://www.cirmmt.org/&quot;&gt;CIRMMT&lt;/a&gt; provided many other opportunities to intersect with people doing thing. This was really valuable not only for learning how to do interdisciplinary research, but also learning how to talk to people from different disciplines.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;EH:&lt;/strong&gt; A lot of what we do at SIMSSA involves symbolic representations of digitized scores, and I know a lot of what you do involves symbolic representations of performance. Could you talk a little bit about making computational models of performance — what is your interest in that, and what sorts of problems do you see that applying to?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;JD:&lt;/strong&gt; In terms of western art music scholarship, there’s a bias towards notated music. But in terms of what people experience, its the performance. Many of questions that we may be asking of the notated music, such as musical structure, can being linked to the listener experience. Thus, it’s useful to be able to examine performances and to see whether the things we find through symbolic analysis are being emphasized by the performer or not. You can have some theories by looking at the score, you can augment those by seeing how people perform it, and you can put those two together in terms of building your reading of the piece.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;EH:&lt;/strong&gt; I know you’re usually at Ohio State and you mentioned you’re at New York University this year. What projects have you been working on and what sorts of things are going on at NYU?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;JD:&lt;/strong&gt; So I usually am in a music theory and cognition program, but I’ve been teaching in the music technology program here this year. I’ve been intersecting with both people doing music cognition — mainly a lot of stuff on emotion and film music actually, I’ve been advising students on that — and people doing music information retrieval. I’ve been continuing some of the work I’ve been doing with my &lt;a href=&quot;http://gettavern.org&quot;&gt;TAVERN dataset&lt;/a&gt; a bit, as well as issues around encoding performance data.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;EH:&lt;/strong&gt; &lt;a href=&quot;http://gettavern.org&quot;&gt;TAVERN&lt;/a&gt; is how I think I first heard about you, but can you give a brief idea of how that project started?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;JD:&lt;/strong&gt; Yeah, so, it originally came about because I managed to convince Google that this was an interesting thing to fund through their Google Faculty Research Award program.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;EH:&lt;/strong&gt; Well done!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;JD:&lt;/strong&gt; Yeah! The grand vision is to develop a model of symbolic music analysis where you’d be parsing the structurally significant notes from the entire musical surface. The first step in developing this develop was to to get some labelled data. I thought that theme and variations form was a really useful way to do this, because, when there’s consistency in terms of the harmony, you can see how the same harmony is being realized with different textures in a way that is totally ecologically valid. So because I was at OSU in music theory, what I actually had available in my graduate students (aka &lt;a href=&quot;https://simssa.ca/blog/introducing-Nat&quot;&gt;Nat&lt;/a&gt; and &lt;a href=&quot;https://simssa.ca/blog/introducing-claire-arthur&quot;&gt;Claire&lt;/a&gt;!) was expertise in doing music analysis, so I used money in order to do the data set creation, which was really a huge undertaking. The next thing is to think about doing an audio version of it.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;EH:&lt;/strong&gt; I wanted to ask you a little about &lt;a href=&quot;http://ampact.tumblr.com/&quot;&gt;AMPACT&lt;/a&gt; since you did that workshop at the Digital Humanities Week in New York recently. Tell me a little bit about the development of AMPACT and the workshop and future plans for it.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://ampact.tumblr.com/&quot;&gt;&lt;img src=&quot;http://localhost:4000/assets/img/ampactLOGO.png&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;JD:&lt;/strong&gt; &lt;a href=&quot;http://ampact.tumblr.com/&quot;&gt;AMPACT&lt;/a&gt; uses scored-guided  techniques to extract timing, tuning, dynamics and tempo from recordings of musical performances. &lt;a href=&quot;http://ampact.tumblr.com/&quot;&gt;AMPACT&lt;/a&gt; really came out of my dissertation — it was the tools that I was using to do the analysis of the vocal performances. The name came out of a solid hour-long meeting with Ich, going through all the possible acronyms. I got funding from the NEH Digital Humanities program and that allowed me to do some additional development, specifically to develop the technology to not just do monophonic performances but polyphonic performances as well. The music performance encoding project also fell under that so that’s how I got funding for that too.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;EH:&lt;/strong&gt; What sorts of people are coming to your workshop to learn about AMPACT?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;JD:&lt;/strong&gt; Since it was the digital humanities event, it was less music scholars and more people who are interested in ethnography of music, but not ethnomusicologists by trade. As well people who are interested in looking at cultural products other than text.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Shown below is a schematic representing the AMPACT process. See &lt;a href=&quot;http://ampact.tumblr.com/documentation&quot;&gt;here&lt;/a&gt; for more.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/ampactSchematic.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;EH:&lt;/strong&gt; The &lt;a href=&quot;https://www.frontiersin.org/journals/digital-humanities#&quot;&gt;Frontiers in Digital Humanities journal&lt;/a&gt;— you’re currently the Specialty Chief Editor for the &lt;a href=&quot;https://www.frontiersin.org/journals/digital-humanities/sections/digital-musicology#&quot;&gt;Digital Musicology&lt;/a&gt; section. Can you tell me a little bit more about that publication, what sorts of work you do and what its goals are?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;JD:&lt;/strong&gt; What I’m most interested in is having a space where it’s not just people using tools that have been developed to do — to answer musicological questions — but rather see the way that musicological questions can actually push the development of tools. And not just MIR tools but actually core algorithm development in fields like machine learning or signal processing.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;EH:&lt;/strong&gt; What have you worked on with machine learning?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;JD:&lt;/strong&gt; I’ve mostly been using classic machine learning approaches as opposed to deep learning techniques, just because the data that I have to work with is relatively small. I’ve worked on classification with support vector machines and music alignment using temporal models like hidden Markov models, and then currently doing some stuff with conditional random fields for the symbolic music analysis from &lt;a href=&quot;http://gettavern.org&quot;&gt;TAVERN&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;EH:&lt;/strong&gt; In terms of learning the technical side of this, how much of your programming knowledge came from your computer programming diploma versus how much have you learned on the job? A lot of musicologists I see learning to code on the job because of getting interested in MIR.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;JD:&lt;/strong&gt; In terms of my actual coding I really learned how to code, like actually properly code, when working on my diploma, and what I’ve been learning since has been stuff like the machine learning and signal processing. I feel like it would be really difficult for me to have gotten a handle on programming paradigms like object-oriented programming on the fly.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;EH:&lt;/strong&gt; Thanks a lot for talking to me today! I appreciate it, I’m glad we have you on board.&lt;/p&gt;

&lt;p&gt;Learn more about Johanna at &lt;a href=&quot;http://devaney.ca&quot;&gt;www.devaney.ca&lt;/a&gt; or follow her on Twitter &lt;a href=&quot;https://twitter.com/jcdevaney&quot;&gt;@jcdevaney&lt;/a&gt;.&lt;/p&gt;</content><author><name>ehopkins</name></author><summary type="html">Johanna Devaney is Assistant Professor of Music Theory and Cognition at Ohio State University, currently teaching in the music technology program at NYU, and the newest SIMSSA Collaborator. We had a chance to talk over Skype about music and the digital humanities, learning to program, and the ways that music research can drive technological advances even beyond disciplinary bounds.</summary></entry><entry><title type="html">Digital Encoding of Mensural Music: an Interview with Martha Thomae</title><link href="http://localhost:4000/blog/interviewing-martha/" rel="alternate" type="text/html" title="Digital Encoding of Mensural Music: an Interview with Martha Thomae" /><published>2018-01-19T00:00:00-05:00</published><updated>2018-01-19T00:00:00-05:00</updated><id>http://localhost:4000/blog/interviewing-martha</id><content type="html" xml:base="http://localhost:4000/blog/interviewing-martha/">&lt;p&gt;&lt;em&gt;Martha Thomae is a PhD student in the Music Tech Department, working on mensural notation for the SIMSSA Project. Originally from Guatemala, she came to Montreal for her Master’s in Music Technology in 2015 and she started her PhD here this past fall. She’s pictured below at the beach in Calahonda, Motril, in the Granada province of Spain. Music Theory PhD student Sam Howes (read our interview with him in 2016 &lt;a href=&quot;http://simssa.ca/blog/from-module-to-schema-an-interview-with-sam-howes&quot;&gt;here&lt;/a&gt;) interviewed her about her work on her Master’s thesis and some more current projects.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/martha-calahonda.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sam Howes:&lt;/strong&gt; Most musicians today have never seen mensural music. Could you give us a little introduction to this method of music notation?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Martha Thomae:&lt;/strong&gt; Sure, mensural music refers to music written in mensural notation, a system that was used between the 1250s and the 1600s for vocal polyphonic music. Mensural notation is the immediate predecessor of our common Western music notation (CWMN) system. You can see already similarities between the two (Figure 1, below). Mensural notation already uses staff lines and clefs to denote pitch, and there are various note shapes used for the different note values. Actually, mensural notation was the system that introduced the idea of using different note shapes to represent distinct values into the Western music tradition.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/martha-1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 1: Example of a mensural piece. Kyrie by J. Barbireau, early sixteenth century. As most mensural music, it is written in separate parts (i.e., the voices are written in different areas of the page). The red circles show the clefs for the first line of each of the four voices of this piece. (Image obtained from Wikipedia)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;But there is one important difference between the two systems and that is that note values in mensural notation are not absolute but rather context-dependent. The same note can have two values: perfect (ternary value, similar to that of a dotted note) and imperfect (duple value, similar to an undotted note). While all notes have a default value given by what is called the “mensuration” (the older idea of meter), the value can be modified by the context (this is, the notes preceding or following the note). The main two context-related modifications are imperfection (when a perfect note is changed to imperfect) and alteration (when a note is twice as long). Principles regarding the contexts in which these modifications take place have been outlined in Franco’s Ars Cantus Mensurabilis (ca. 1280), these are known as principles of imperfection and alteration. Figure 2 (below) shows an example of such modifications.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/martha-2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 2: In this excerpt the long (blue box) is perfect by default, which means it is equivalent to three breves (purple box). The barlines were added to divide the notes into groups equivalent to three breves. The blue and purple boxes show the default values of the long and the breve, respectively. On the other hand, the green and red boxes show their contextually modified durations. The long in the green box is an example of imperfection (i.e., the perfect long losses one third of its value and becomes imperfect) and the breve in the red box is an example of alteration (i.e., the note’s value gets doubled).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;SH:&lt;/strong&gt; Please tell us about the music you are working with. Where and when are these manuscripts from?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MT:&lt;/strong&gt; I have been working with &lt;a href=&quot;http://www.arsmusicae.org/wordpress/about/&quot;&gt;Prof. Karen Desmond&lt;/a&gt; on the digital encoding of French motets from the 1300s to the 1350s. This time span corresponds to a period of stylistic change, between the ars antiqua (old art) and the ars nova (new art) styles of mensural notation. Thus, the repertoire we are working with has features of both the old and the new art. We are working with motets from four manuscript sources: the Montpellier Codex, the Roman de Fauvel manuscript, the Brussels rotulus, and the Ivrea Codex. Recently I expanded this set of encoded pieces to include a few fifteenth-century chansons by Du Fay and Ockeghem.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;SH:&lt;/strong&gt; How did you become interested in mensural notation and when did you start working with it?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MT:&lt;/strong&gt; I became interested in mensural notation during the last year of my undergrad. I was studying a BSc in Mathematics and I wanted my Undergraduate thesis to be multidisciplinary, specifically I wanted it to be applicable to music. One of my professors talked to me about mensural notation and about how its context-dependent nature makes it hard to transcribe it into modern values. I wanted to develop an algorithm that modeled the transcription process of mensural notation into modern values, so I took Willi Apel’s The Notation of Polyphonic Music 900–1600 and began studying this notational system.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;SH:&lt;/strong&gt; Why is it important to have digital representations of mensural music? How will these benefit music researchers?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MT:&lt;/strong&gt; The encoding of music in machine-readable files provides a way to preserve and share the music, but, most importantly, it allows for computational music-analysis. We can develop scripts, using music-analysis libraries such as &lt;a href=&quot;http://web.mit.edu/music21/&quot;&gt;music21&lt;/a&gt;, to search for various features to analyze a piece (e.g., the types of sonorities on every downbeat). Actually, the motets from the 1300–1350 transition period were used in Karen Desmond’s Measuring Polyphony Project to study the change in the deployment of sonority over this period. With encoded music, we can answer similar questions and perform large corpus studies, we can actually prove (or disprove) our hypothesis regarding a piece or set of pieces from a particular composer, style, or period.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;SH:&lt;/strong&gt; What are some of the challenges in the encoding process? How are you dealing with these?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MT:&lt;/strong&gt; First, there are very few formats that provide support for encoding mensural notation. The one I have been working with for the past two years is &lt;a href=&quot;http://music-encoding.org/&quot;&gt;MEI (Music Encoding Initiative)&lt;/a&gt;, which supports mensural notation through its Mensural MEI module. MEI is a flexible format for encoding a wide variety of music (including mensural), but the Mensural module is still a work in progress. This presented a difficulty when dealing with mensural features whose encoding has not yet been standardized.&lt;/p&gt;

&lt;p&gt;But the most significant issue for encoding mensural music is capturing the rhythmic information. In CWMN the relative duration of a note is explicitly stated by the notation; we only need to encode the shape of the note and the fact that it is dotted or not (or that it is part of a tuplet) to account for the durational information. This is not the case in mensural notation: the note shape, although necessary, is not sufficient to determine the duration of a mensural note because it depends on the context.&lt;/p&gt;

&lt;p&gt;I have worked in two projects that deal with this context-dependency issue in distinct ways:&lt;/p&gt;

&lt;p&gt;First, Karen Desmond’s Measuring Polyphony Project. In this project, we deal with the context-dependent nature of note duration by extracting the duration information of the notes of a mensural piece from a modern transcription of the piece (this is, a transcription of the piece into modern values). We are taking modern transcriptions of the repertoire and translate them back into mensural notation. The modern transcriptions are entered into Sibelius and articulation marks are added to account for mensural notation specificities that are not generally included in modern transcriptions (Figure 3, below).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/martha-3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 3: Example of an edited modern transcription (bottom) of a mensural piece (top). The articulation marks of the modern transcription are a way to account for mensural notation specificities that do not exist in CWMN, such as plicas (purple and pink circles), dots of division (red arrows), downward stems (green box) and alterations.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Through a series of processes, these edited-Sibelius files are transformed into symbolic files that encode the piece in the original mensural notation (Figure 4, below).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/martha-4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 4: Three-step methodology to obtain the Mensural MEI file that encodes the mensural piece in the original notation and with the right contextual duration of all of its notes. First step, entering a modern transcription into Sibelius (which can be done by importing a midi or MusicXML file) and adding articulation marks to account for mensural notation specificities. Second step, use of the SibMEI plugin to obtain CMN MEI file that encodes the edited modern transcription in common music notation. Third step, use the Mensural MEI Translator to translate the encoding of the modern transcription into the encoding of the original mensural piece, which is stored in a Mensural MEI file.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Second, the &lt;a href=&quot;https://github.com/ELVIS-Project/scoring-up&quot;&gt;scoring-up tool&lt;/a&gt; I developed for my MA thesis. In this project, the program itself figures out the duration of the notes by using the principles of imperfection and alteration summarized in Franco’s Ars Cantus Mensurabilis. The result of the program is a file that contains all the information for the mensural piece to be rendered in the original notation but in score format (rather than the usual separate-parts layout of the original piece).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/martha-5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 5: The scoring-up tool takes a set of Mensural MEI files that encode each of the voices of the piece, including only pitch and note shape information. These input files could be obtained by performing Optical Music Recognition (i.e., computer recognition of music glyphs) on the original sources. The output consists of a single file that encodes all the information of the input files plus the duration of all notes, representing the piece as a mensural score.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;SH:&lt;/strong&gt; In terms of developing your own software, what is your main goal? How will your software interact with other applications that already exist?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MT:&lt;/strong&gt; My main goal is to develop a tool that facilitates the encoding of mensural pieces so that they are faithful to the original sources (they are written in the original notation) but that also include information regarding the notes’ contextual duration. The two projects I have been working on relate to this goal.&lt;/p&gt;

&lt;p&gt;The software I developed for the Measuring Polyphony Project, called the &lt;a href=&quot;https://github.com/DDMAL/CMN-MEI_to_MensuralMEI_Translator&quot;&gt;Mensural MEI Translator&lt;/a&gt;, is used in conjunction with other MEI applications:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://music-encoding.org/tools/sibmei/&quot;&gt;SibMEI&lt;/a&gt; a plugin that converts a Sibelius file into a MEI file in CWMN values&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://music-encoding.org/tools/libmei/&quot;&gt;LibMEI&lt;/a&gt;, a library for interacting with MEI files&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.verovio.org/index.xhtml&quot;&gt;Verovio&lt;/a&gt;, a render engine for MEI files which allows for displaying the resulting mensural piece on a web-browser&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The scoring-up tool is also built upon LibMEI, and the resulting mensural score is visible when this file is rendered in Verovio.&lt;/p&gt;

&lt;p&gt;Thank you!&lt;/p&gt;</content><author><name>ehopkins</name></author><summary type="html">Martha Thomae is a PhD student in the Music Tech Department, working on mensural notation for the SIMSSA Project. Originally from Guatemala, she came to Montreal for her Master’s in Music Technology in 2015 and she started her PhD here this past fall. She’s pictured below at the beach in Calahonda, Motril, in the Granada province of Spain. Music Theory PhD student Sam Howes (read our interview with him in 2016 here) interviewed her about her work on her Master’s thesis and some more current projects.</summary></entry></feed>

@article{roland_early_2014,
	title = {Early music and the {Music} {Encoding} {Initiative}.},
	volume = {42},
	language = {en},
	journal = {Early Music},
	author = {Roland, Perry and Hankinson, Andrew and Pugin, Laurent},
	year = {2014},
	pages = {605--11}
}

@phdthesis{hankinson_optical_2014,
	address = {Montreal, Canada},
	type = {{PhD} diss.},
	title = {Optical music recognition infrastructure for large-scale music document analysis.},
	language = {es},
	school = {Schulich School of Music, McGill University},
	author = {Hankinson, Andrew},
	year = {2014}
}

@incollection{burgoyne_compositional_2013,
	address = {Montreal, QC},
	series = {Lecture {Notes} in {Artificial} {Intelligence} 7937},
	title = {Compositional {Data} {Analysis} of {Harmonic} {Structures} in {Popular} {Music}.},
	language = {en},
	booktitle = {Proceedings of the {International} {Conference} on {Mathematics} and {Computation} in {Music}},
	author = {Burgoyne, John Ashley and Wild, Jon and Fujinaga, Ichiro},
	year = {2013},
	pages = {52--63}
}

@inproceedings{winters_sonification_2014,
	address = {New York, USA},
	title = {Sonification of {Symbolic} {Music} in the {Elvis} {Project}},
	language = {en},
	booktitle = {Proceedings of the 20th {International} {Conference} on {Auditory} {Display} ({ICAD}–2014)},
	author = {Winters, R. Michael and Cumming, Julie E.},
	month = jun,
	year = {2014},
	note = {https://simssa.ca/assets/files/WintersSchulichSonification.pdf}
}

@inproceedings{pugin_verovio:_2014,
	address = {Taipei, Taiwan},
	title = {Verovio: {A} library for engraving {MEI} music notation into {SVG}},
	language = {en},
	booktitle = {Proceedings of the 15th {International} {Society} for {Music} {Information} {Retrieval} {Conference} ({ISMIR})},
	author = {Pugin, Laurent and Zitellini, Rodolfo and Roland, Perry},
	year = {2014},
	pages = {107--12}
}

@article{helsen_optical_2014,
	title = {Optical {Music} {Recognition} and {Manuscript} {Chant} {Sources}},
	volume = {42},
	language = {en},
	journal = {Early Music},
	author = {Helsen, Kate and Bain, Jennifer and Fujinaga, Ichiro and Hankinson, Andrew and Lacoste, Debra},
	year = {2014},
	pages = {555--58}
}

@inproceedings{hankinson_diva:_2012,
	address = {Heidelberg},
	title = {Diva: {A} web-based document image viewer},
	language = {en},
	booktitle = {Proceedings of the {Conference} on {Theory} and {Practice} in {Digital} {Libraries}},
	publisher = {Springer},
	author = {Hankinson, Andrew and Liu, Wendy and Pugin, Laurent and Fujinaga, Ichiro},
	year = {2012},
	note = {https://simssa.ca/assets/files/liu12diva.pdf}
}

@inproceedings{hankinson_digital_2012,
	address = {Porto, Portugal},
	title = {Digital document image retrieval using optical music recognition},
	url = {http://cloud.simssa.ca/index.php/s/YN1gYHw0mRor9rE},
	language = {en},
	booktitle = {Proceedings of 13th {International} {Society} for {Music} {Information} {Retrieval} {Conference} ({ISMIR})},
	author = {Hankinson, Andrew and Burgoyne, John Ashley and Vigliensoni, Gabriel and Porter, Alastair and Thompson, Jessica and Liu, Wendy and Chiu, Remi and Fujinaga, Ichiro},
	year = {2012}
}

@incollection{cumming_past_2014,
	address = {Montreal},
	title = {The {Past} is not {Over}: {Special} {Collections} in the {Digital} {Age}},
	language = {en},
	booktitle = {Meetings with {Books}: {Symposium} on {Special} {Collections} in the 21st {Century}. {With} a {Tribute} to {Raymond} {Klibansky} and an {Illustrated} {Survey} of {McGill} {Library} {Special} {Collections}},
	publisher = {McGill University Library},
	author = {Cumming, Julie},
	editor = {Tomm, Jillian and Virr, Richard},
	year = {2014},
	note = {https://simssa.ca/assets/files/CummingSpecialCollections.pdf},
	pages = {109--14}
}

@incollection{charalampos_correcting_2014,
	address = {London, UK},
	title = {Correcting large­-scale {OMR} data with crowdsourcing},
	isbn = {test.com},
	language = {en},
	booktitle = {Proceedings of the {International} {Workshop} on {Digital} {Libraries} for {Musicology}},
	author = {Charalampos, Saitis and Hankinson, Andrew and Fujinaga, Ichiro},
	year = {2014},
	keywords = {crowd-sourcing, OMR},
	pages = {88--90}
}

@inproceedings{antila_vis_2014,
	address = {Taipei, Taiwan},
	title = {The {VIS} framework: {Analyzing} counterpoint in large datasets},
	language = {en},
	booktitle = {Proceedings of the 15th {International} {Society} for {Music} {Information} {Retrieval} {Conference} ({ISMIR})},
	author = {Antila, Christopher and Cumming, Julie},
	year = {2014},
	note = {https://simssa.ca/assets/files/AntilaCummingVISFramework.pdf}
}

@incollection{burlet_neon.js:_2012,
	address = {Porto, Portugal},
	title = {Neon.js: {Neume} {Editor} {Online}},
	language = {en},
	booktitle = {Proceedings of the 13th {International} {Society} for {Music} {Information} {Retrieval} {Conference} ({ISMIR})},
	author = {Burlet, Gregory and Porter, Alastair and Hankinson, Andrew and Fujinaga, Ichiro},
	year = {2012},
	note = {https://simssa.ca/assets/files/burlet12neon.pdf},
	pages = {121--26}
}

@inproceedings{lacoste_chants_2016,
	address = {Montreal, QC},
	title = {Chants that {Defy} {Classification}: {The} {Implications} of {Categorization} in the {Cantus} {Database}},
	booktitle = {Proceedings of the {Music} {Encoding} {Conference} (2015-2017)},
	publisher = {Music Encoding Initiative},
	author = {Lacoste, Debra and Swanson, Barbara},
	year = {2016},
	pages = {73--78}
}

@inproceedings{kramer_intermediate_2016,
	address = {McGill University, Montreal, QC},
	title = {On {Intermediate} {Formats}},
	booktitle = {Proceedings of the {Music} {Encoding} {Conference} (2015-2017)},
	author = {Krämer, Reiner},
	month = may,
	year = {2016},
	pages = {53--62}
}

@incollection{morgan_tacit_2018,
	title = {The {Tacit} {Principles} of {Tinctoris}'s {Interval} {Successions}},
	booktitle = {Johannes {Tinctoris} and {Music} {Theory}},
	publisher = {Brepols},
	author = {Morgan, Alexander},
	editor = {Woodley, Ronald},
	year = {2018}
}

@inproceedings{hankinson_interchange_2010,
	address = {Utrecht, NL},
	title = {An interchange format for optical music recognition applications.},
	language = {en},
	booktitle = {Proceedings of the 11th {International} {Society} for {Music} {Information} {Retrieval} {Conference} ({ISMIR})},
	author = {Hankinson, Andrew and Pugin, Laurent and Fujinaga, Ichiro},
	year = {2010}
}

@inproceedings{hankinson_interfaces_2009,
	address = {Kobe, JP},
	title = {Interfaces for {Document} {Representation} in {Digital} {Music} {Libraries}},
	language = {en},
	booktitle = {Proceedings of the 10th {International} {Society} for {Music} {Information} {Retrieval} {Conference} ({ISMIR})},
	author = {Hankinson, Andrew and Pugin, Laurent and Fujinaga, Ichiro},
	year = {2009}
}

@inproceedings{vigliensoni_optical_2013,
	address = {Curitiba, Brazil},
	title = {Optical {Measure} {Recognition} in {Common} {Music} {Notation}},
	url = {http://cloud.simssa.ca/index.php/s/e91SMTDZgP7XtNd},
	language = {en},
	booktitle = {Proceedings of the 14th {International} {Society} for {Music} {Information} {Retrieval} {Conference} ({ISMIR})},
	author = {Vigliensoni, Gabriel and Burlet, Gregory and Fujinaga, Ichiro},
	year = {2013},
	pages = {125--30}
}

@inproceedings{vigliensoni_automatic_2011,
	address = {Miami, FL},
	title = {Automatic pitch detection in printed square notation},
	url = {http://cloud.simssa.ca/index.php/s/yxAuJOiZau6LMOL},
	language = {en},
	booktitle = {Proceedings of the 12th {International} {Society} for {Music} {Information} {Retrieval} {Conference} ({ISMIR})},
	author = {Vigliensoni, Gabriel and Burgoyne, John Ashley and Hankinson, Andrew and Fujinaga, Ichiro},
	year = {2011},
	pages = {423--28}
}

@inproceedings{sigler_schematizing_2015,
	title = {Schematizing the {Treatment} of {Dissonance} in 16th-{Century} {Counterpoint}},
	booktitle = {Proceedings of the 16th {International} {Society} for {Music} {Information} {Retrieval} {Conference} ({ISMIR})},
	author = {Sigler, Andie and Wild, Jon and Handelman, Eliot},
	month = oct,
	year = {2015}
}

@inproceedings{hankinson_music_2011,
	address = {Miami, FL},
	title = {The {Music} {Encoding} {Initiative} as a {Document}-{Encoding} {Framework}},
	language = {en},
	booktitle = {Proceedings of the 12th {International} {Society} for {Music} {Information} {Retrieval} {Conference} ({ISMIR})},
	author = {Hankinson, Andrew and Roland, Perry and Fujinaga, Ichiro},
	year = {2011},
	note = {https://simssa.ca/assets/files/HankinsonIsmir.pdf},
	pages = {293--98}
}

@inproceedings{calvo-zaragoza_end--end_2017,
	address = {Suzhou, China},
	title = {End-to-{End} {Optical} {Music} {Recognition} {Using} {Neural} {Networks}},
	booktitle = {Proceedings of the 18th {International} {Society} for {Music} {Information} {Retrieval} {Conference} ({ISMIR})},
	author = {Calvo-Zaragoza, Jorge and Valero-Mas, Jose Javier and Pertusa, Antonio},
	year = {2017}
}

@inproceedings{risk_computational_2015,
	title = {Computational {Ranking} of {Melodic} {Similarity} in {French}-{Canadian} {Instrumental} {Dance} {Tunes}.},
	language = {en},
	booktitle = {Proceedings of the 16th {International} {Society} for {Music} {Information} {Retrieval} ({ISMIR})},
	author = {Risk, Laura and Mok, Lillio and Hankinson, Andrew and Cumming, Julie},
	month = oct,
	year = {2015}
}

@inproceedings{laplante_digitizing_2016,
	title = {Digitizing musical scores: {Challenges} and opportunities for libraries},
	doi = {http://dx.doi.org/10.1145/2970044.2970055},
	booktitle = {Proceedings of the 3rd {International} {Workshop} on {Digital} {Libraries} for {Musicology}},
	author = {Laplante, Audrey and Fujinaga, Ichiro},
	year = {2016}
}

@inproceedings{vigliensoni_identifying_2014,
	address = {Gold Coast, Australia},
	title = {Identifying time zones in a large dataset of music listening logs},
	url = {http://cloud.simssa.ca/index.php/s/XHUtAQXBI8zEOyx},
	language = {en},
	booktitle = {Proceedings of the {International} {Workshop} on {Social} {Media} {Retrieval} and {Analysis}},
	author = {Vigliensoni, Gabriel and Fujinaga, Ichiro},
	year = {2014},
	pages = {27--32}
}

@inproceedings{hankinson_decentralized_2015,
	address = {Vancouver, BC},
	title = {Decentralized {Music} {Document} {Image} {Searching} with {Optical} {Music} {Recognition} and the {International} {Image} {Operability} {Framework}.},
	booktitle = {Proceedings of the {Digital} {Library} {Federation} {Forum}},
	author = {Hankinson, Andrew and Magoni, Evan and Fujinaga, Ichiro},
	month = oct,
	year = {2015},
	note = {https://simssa.ca/assets/files/hankinson-decentralized-dlf2015.pdf}
}

@inproceedings{hankinson_accessing_2014,
	address = {Lausanne, Switzerland},
	title = {Accessing, {Navigating}, and {Engaging} with {High}-{Resolution} {Document} {Image} {Collections} {Using} {Diva}.js},
	booktitle = {Conference {Abstracts} of {Digital} {Humanities}},
	author = {Hankinson, Andrew and Fujinaga, Ichiro},
	year = {2014},
	pages = {186--8}
}

@inproceedings{hankinson_creating_2012,
	address = {Lyon, FR},
	title = {Creating a large-scale searchable digital collection from printed music materials.},
	url = {http://cloud.simssa.ca/index.php/s/g4kY4j3CzEa6vT6},
	language = {en},
	booktitle = {Proceedings of the {World} {Wide} {Web} {Conference}},
	author = {Hankinson, Andrew and Burgoyne, John Ashley and Vigliensoni, Gabriel and Fujinaga, Ichiro},
	year = {2012},
	pages = {903--908}
}

@inproceedings{fujinaga_big_2014,
	title = {Big data for the music perception and cognition community.},
	language = {en},
	booktitle = {Book of {Abstracts} of {International} {Conference} on {Music} {Perception} and {Cognition} - {Asia}-{Pacific} {Society} for the {Cognitive} {Sciences} of {Music} {Joint} {Conference}},
	publisher = {Seoul, South Korea},
	author = {Fujinaga, Ichiro and Sears, David and Hankinson, Andrew},
	year = {2014},
	pages = {78}
}

@inproceedings{fujinaga_single_2015,
	address = {Paris, France},
	title = {Single interface for music score searching and analysis ({SIMSSA})},
	booktitle = {Proceedings of the {First} {International} {Conference} on {Technologies} for {Music} {Notation} and {Representation}},
	author = {Fujinaga, Ichiro and Hankinson, Andrew},
	year = {2015}
}

@inproceedings{fujinaga_simssa:_2013,
	address = {Lincoln, NE},
	title = {{SIMSSA}: {Towards} {Full}-{Music} {Search} {Over} a {Large} {Collection} of {Musical} {Scores}.},
	language = {en},
	booktitle = {Conference {Abstracts} of {Digital} {Humanities}},
	author = {Fujinaga, Ichiro and Hankinson, Andrew},
	year = {2013},
	pages = {187--89}
}

@incollection{fujinaga_automatic_2018,
	address = {Heidelberg},
	title = {Automatic score extraction with optical music recognition},
	booktitle = {Current {Research} in {Systematic} {Musicology}, {R}. {Bader}, {M}. {Leman}, {R}. {Godoy}, eds.},
	publisher = {Springer},
	author = {Fujinaga, Ichiro and Hankinson, Andrew and Pugin, Laurent},
	year = {2018},
	pages = {299--311}
}

@inproceedings{fujinaga_introduction_2014,
	address = {London, UK},
	title = {Introduction to {SIMSSA} ({Single} {Interface} for {Music} {Score} {Searching} and {Analysis})},
	language = {en},
	booktitle = {Proceedings of the {International} {Workshop} on {Digital} {Libraries} for {Musicology}},
	author = {Fujinaga, Ichiro and Hankinson, Andrew and Cumming, Julie},
	year = {2014},
	pages = {100--102}
}

@inproceedings{calvo-zaragoza_pixel-wise_2017,
	address = {Nagoya, Japan},
	title = {Pixel-{Wise} {Binarization} of {Musical} {Document} with {Convolutional} {Neural} {Networks}},
	url = {http://cloud.simssa.ca/index.php/s/17ZXOEHz87iIzHM},
	booktitle = {Proceedings of the 15th {IAPR} {International} {Conference} on {Machine} {Vision} {Applications}},
	author = {Calvo-Zaragoza, Jorge and Vigliensoni, Gabriel and Fujinaga, Ichiro},
	year = {2017},
	pages = {362--65}
}

@incollection{bain_linienlose_2014,
	address = {Wiesbaden},
	series = {Trierer {Beiträge} zu den historischen {Kulturwissenschaften}},
	title = {Linienlose {Neumen}, {Neumentrennung} und {Repräsentation} von {Neumen} mit {MEI} {Schema}–{Herausforderungen} in der {Arbeit} im {Optical} {Neume} {Recognition} {Project} ({ONRP}).},
	language = {de},
	number = {12},
	booktitle = {Digitale {Rekonstruktionen} mittelalterlicher {Bibliotheken}},
	publisher = {Ludwig Reichert},
	author = {Bain, Jennifer and Behrendt, Inga and Helsen, Katherine},
	editor = {Philippi, Sabine and Vanscheidt, Philipp},
	year = {2014},
	pages = {119--32}
}

@mastersthesis{yang_singing_2016,
	address = {Montreal, QC},
	title = {Singing transcription using machine learning with feature selection},
	school = {McGill University},
	author = {Yang, Ling-Xiao},
	year = {2016}
}

@phdthesis{vigliensoni_evaluating_2017,
	address = {Montreal, QC},
	type = {{PhD} diss.},
	title = {Evaluating the performance improvement of a music recommendation model by using user-centric features},
	url = {http://cloud.simssa.ca/index.php/s/QHcj2J9rQfM9fm8},
	school = {McGill University},
	author = {Vigliensoni, Gabriel},
	year = {2017}
}

@inproceedings{baro_optical_2017,
	address = {Kyoto, Japan},
	title = {Optical {Music} {Recognition} by {Recurrent} {Neural} {Networks}},
	booktitle = {Proceedings of the {Twelfth} {IAPR} {International} {Workshop} on {Graphics} {Recognition}},
	publisher = {Springer LNCS},
	author = {Baró, Arnau and Riba, Pau and Calvo-Zaragoza, Jorge and Fornés, Alicia},
	year = {2017}
}

@inproceedings{saleh_pixel.js:_2017,
	address = {Kyoto, Japan},
	title = {Pixel.js: {Web}-based {Pixel} {Classification} {Correction} {Platform} for {Ground} {Truth} {Creation}},
	url = {http://cloud.simssa.ca/index.php/s/7rWLN8VW6lXL8i7},
	booktitle = {Proceedings of the {Twelfth} {IAPR} {International} {Workshop} on {Graphics} {Recognition}},
	publisher = {Springer LNCS},
	author = {Saleh, Zeyad and Zhang, Ké and Calvo-Zaragoza, Jorge and Vigliensoni, Gabriel and Fujinaga, Ichiro},
	year = {2017},
	pages = {2:39--40}
}

@inproceedings{garfinkle_patternfinder:_2017,
	address = {Shanghai, China},
	title = {{PatternFinder}: {Content}-{Based} {Music} {Retrieval} with music21},
	booktitle = {Proceedings of the 4th {International} {Workshop} on {Digital} {Libraries} for {Musicology}},
	author = {Garfinkle, David and Schubert, Peter and Arthur, Claire and Cumming, Julie and Fujinaga, Ichiro},
	year = {2017}
}

@inproceedings{ju_non-chord_2017,
	address = {Shanghai, China},
	title = {Non-chord {Tone} {Identification} {Using} {Deep} {Neural} {Networks}},
	booktitle = {Proceedings of the {International} {Workshop} on {Digital} {Libraries} for {Musicology}},
	author = {Ju, Yaolong and Condit-Schultz, Nathaniel and Arthur, Claire and Fujinaga, Ichiro},
	year = {2017},
	pages = {13--16}
}

@book{desmond_music_2018,
	title = {Music and the moderni, 1300-1500: {The} ars nova in theory and practice},
	publisher = {Cambridge University Press},
	author = {Desmond, Karen},
	year = {2018}
}

@phdthesis{risk_veillees_2017,
	address = {Montreal},
	type = {{PhD} diss.},
	title = {Veillées, {Variants}, and {Violoneux}: {Generic} boundaries and transnational trajectories in the traditional instrumental music of {Quebec}},
	school = {McGill University},
	author = {Risk, Laura},
	year = {2017}
}

@phdthesis{morgan_renaissance_2017,
	address = {Montreal},
	type = {{PhD} diss.},
	title = {Renaissance interval-succession theory: treatises and analysis},
	school = {McGill University},
	author = {Morgan, Alexander},
	year = {2017}
}

@mastersthesis{thomae_automatic_2017,
	address = {Montreal},
	title = {Automatic {Scoring} up of {Mensural} {Music} {Using} {Perfect} {Mensurations}, 1300-1550},
	school = {McGill University},
	author = {Thomae, Martha E.},
	year = {2017}
}

@incollection{cumming_origins_2015,
	address = {Cambridge, UK},
	title = {The {Origins} of {Pervasive} {Imitation}},
	language = {en},
	booktitle = {The {Cambridge} {History} of {Fifteenth}-{Century} {Music}},
	publisher = {Cambridge University Press},
	author = {Cumming, Julie and Schubert, Peter},
	editor = {Berger, Anna Maria Busse and Rodin, Jesse},
	year = {2015}
}

@article{schubert_another_2015,
	title = {Another {Lesson} from {Lassus}: using computers to analyse counterpoint},
	volume = {43},
	language = {en},
	number = {4},
	journal = {Early Music},
	author = {Schubert, Peter and Cumming, Julie},
	month = nov,
	year = {2015},
	pages = {577--86}
}

@inproceedings{lacoste_cantus_2011,
	address = {Barnard College},
	title = {The {Cantus} {Database}: {Mining} for {Medieval} {Chant} {Traditions}},
	volume = {7},
	booktitle = {Digital {Medievalist}},
	author = {Lacoste, Debra},
	year = {2011}
}

@inproceedings{lacoste_renewal_2012,
	address = {Vienna},
	title = {Renewal, {Revival}, {Rejuvenation}: a {New} {Vision} for the {Cantus} {Database}},
	booktitle = {Cantus {Planus}: {Study} {Group} of the {International} {Musicological} {Society} - {Papers} read at the 16th meeting, {Vienna}, {Austria}, 2011},
	publisher = {Verlag Brüder Hollinek},
	author = {Lacoste, Debra and Koláček, Jan},
	year = {2012},
	pages = {202--209}
}

@inproceedings{lacoste_cantus:_2013,
	address = {Lions Bay, BC, Canada},
	title = {{CANTUS}: {A} {Database} for {Latin} {Ecclesiastical} {Chant} - {Progress} {Report} (2009)},
	booktitle = {Papers {Read} at the 15th {Meeting} of the {IMS} {Study} {Group} '{Cantus} {Planus}', {DobogókÅ}/{Hungary}, 2009. {Aug}. 23-29},
	publisher = {The Institute of Medieval Music},
	author = {Lacoste, Debra},
	year = {2013},
	pages = {939--43}
}

@inproceedings{lacoste_cantus_2014,
	address = {Venice, Italy},
	title = {{CANTUS} for {Office} and {Mass}: {Building} an {Online} {Network} of {Chant} {Databases}},
	booktitle = {Cantus {Planus}: {Study} {Group} of the {International} {Musicological} {Society} -- {Papers} read at the 18th meeting, {Venice}, {Italy}, 2014},
	author = {Lacoste, Debra and Koláček, Jan},
	year = {2014}
}

@article{helsen_report_2011,
	title = {A {Report} on the {Encoding} of {Melodic} {Incipits} in the {Cantus} {Database} with the {Music} {Font} ‘'{Volpiano}’},
	volume = {20},
	url = {http://journals.cambridge.org/action/displayAbstract?fromPage=online&aid=8219285},
	number = {1},
	journal = {Plainsong \& Medieval Music},
	author = {Helsen, Kate and Lacoste, Debra},
	year = {2011},
	pages = {51--65}
}

@article{pedersoli_document_2016,
	title = {Document segmentation and classification into musical scores and text},
	volume = {19},
	number = {4},
	journal = {International Journal on Document Analysis and Recognition (IJDAR)},
	author = {Pedersoli, Fabrizio and Tzanetakis, George},
	year = {2016},
	pages = {289--304}
}

@article{behrendt_mei_2017,
	title = {{MEI} {Kodierung} der frühesten {Notation} in linienlosen {Neumen}},
	volume = {4},
	journal = {Kodikologie und Paläographie im Digitalen Zeitalter / Codicology and Paleography in the Digital Age},
	author = {Behrendt, Inga and Bain, Jennifer and Helsen, Kate},
	editor = {Busch, Hannah and Fischer, Franz and Sahle, Patrick},
	year = {2017},
	pages = {281--296}
}

@inproceedings{hankinson_building_2017,
	address = {Montreal, QC},
	title = {Building the new {DIAMM}: {Linking} and sharing data for medieval musicology},
	booktitle = {Conference {Abstracts} of {Digital} {Humanities}},
	publisher = {Alliance of Digital Humanities Organizations},
	author = {Hankinson, Andrew and Craig-McFeely, Julia},
	month = aug,
	year = {2017},
	pages = {69--73}
}

@incollection{arthur_role_2018,
	address = {Oxford, UK},
	title = {The {Role} of {Structural} {Tones} in {Establishing} {Mode} in {Renaissance} {Counterpoint}},
	booktitle = {Oxford {Companion} to {Corpus} {Studies}},
	publisher = {Oxford University Press},
	author = {Arthur, Claire and Cumming, Julie and Schubert, Peter},
	editor = {Shanahan, Daniel and Burgoyne, Ashley and Quinn, Ian},
	year = {2018}
}

@inproceedings{mckay_using_2017,
	address = {Prague, Czech Republic},
	title = {Using statistical feature extraction to distinguish the styles of different composers},
	booktitle = {Abstracts of the {Annual} {International} {Medieval} and {Renaissance} {Music} {Conference}},
	publisher = {MedRen},
	author = {McKay, Cory and Tenaglia, Tristano and Cumming, Julie and Fujinaga, Ichiro},
	month = jul,
	year = {2017},
	pages = {111}
}

@inproceedings{horwitz_browser-based_2015,
	address = {Florence, Italy},
	title = {A browser-based {MEI} editor},
	booktitle = {Proceedings of the {Music} {Encoding} {Conference} (2015-2017)},
	publisher = {Music Encoding Initiative},
	author = {Horwitz, Andrew and Hankinson, Andrew and Fujinaga, Ichiro},
	month = may,
	year = {2015},
	pages = {45--46}
}

@inproceedings{pacha_optical_2018,
	address = {Paris, France},
	title = {Optical {Music} {Recognition} in {Mensural} {Notation} with {Region}-based {Convolutional} {Neural} {Networks}},
	booktitle = {Proceedings of the 19th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Pacha, Alexander and Calvo-Zaragoza, Jorge},
	month = sep,
	year = {2018},
	pages = {240--247}
}

@inproceedings{condit-schultz_flexible_2018,
	address = {Paris, France},
	title = {A {Flexible} {Approach} to {Automated} {Harmonic} {Analysis}: {Multiple} {Annotations} of {Chorales} by {Bach} and {Prætorius}},
	booktitle = {Proceedings of the 19th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	publisher = {ISMIR},
	author = {Condit-Schultz, Nat and Ju, Yaolong and Fujinaga, Ichiro},
	year = {2018},
	pages = {66--73}
}

@inproceedings{mckay_jsymbolic_2018,
	address = {Paris, France},
	title = {{jSymbolic} 2.2: {Extracting} {Features} from {Symbolic} {Music} for use in {Musicological} and {MIR} {Research}},
	booktitle = {Proceedings of the 19th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	publisher = {ISMIR},
	author = {McKay, Cory and Cumming, Julie and Fujinaga, Ichiro},
	month = sep,
	year = {2018},
	pages = {348--54}
}

@inproceedings{roman_end--end_2018,
	address = {Paris, France},
	title = {An {End}-to-end {Framework} for {Audio}-to-{Score} {Music} {Transcription} on {Monophonic} {Excerpts}},
	booktitle = {Proceedings of the 19th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Román, Miguel A. and Pertusa, Antonio and Calvo-Zaragoza, Jorge},
	month = sep,
	year = {2018}
}

@article{calvo-zaragoza_deep_2018,
	title = {Deep {Neural} {Networks} for {Document} {Processing} of {Music} {Score} {Images}},
	volume = {8},
	url = {http://cloud.simssa.ca/index.php/s/KYJaxAWMJRC614L},
	doi = {https://doi.org/10.3390/app8050654},
	number = {5},
	journal = {Applied Sciences},
	author = {Calvo-Zaragoza, Jorge and Castellanos, Francisco J. and Vigliensoni, Gabriel and Fujinaga, Ichiro},
	year = {2018},
	pages = {654}
}

@article{long_international_2018,
	title = {International {Image} {Interoperability} {Framework}; {Gallica}; e-codices: {Virtual} {Manuscript} {Library} of {Switzerland}},
	volume = {71},
	number = {2},
	journal = {Journal of the American Musicological Society},
	author = {Long, Sarah Ann},
	year = {2018},
	pages = {561--571}
}

@inproceedings{vigliensoni_environment_2018,
	address = {Tokyo, Japan},
	title = {An {Environment} for {Machine} {Pedagogy}: {Learning} {How} to {Teach} {Computers} to {Read} {Music}},
	url = {http://cloud.simssa.ca/index.php/s/bBK9N6gQTpPaKkl},
	booktitle = {Joint {Proceedings} of the {ACM} {Intelligent} {User} {Interface} {Workshops}: {Intelligent} {Music} {Interfaces} for {Listening} and {Creation} ({MILC})},
	author = {Vigliensoni, Gabriel and Calvo-Zaragoza, Jorge and Fujinaga, Ichiro},
	year = {2018}
}

@inproceedings{calvo-zaragoza_machine_2017,
	address = {La Coruña, Spain},
	title = {A {Machine} {Learning} {Framework} for the {Categorization} of {Elements} in {Images} of {Musical} {Documents}},
	url = {http://cloud.simssa.ca/index.php/s/NExq8IWkue2IjCH},
	booktitle = {Proceedings of the {Third} {International} on {Technologies} for {Music} {Notation} and {Representation} ({TENOR} 2017)},
	author = {Calvo-Zaragoza, Jorge and Vigliensoni, Gabriel and Fujinaga, Ichiro},
	year = {2017}
}

@inproceedings{calvo-zaragoza_one-step_2017,
	address = {Suzhou, China},
	title = {One-{Step} {Detection} of {Background}, {Staff} {Lines}, and {Symbols} in {Medieval} {Music} {Manuscripts} with {Convolutional} {Neural} {Networks}},
	url = {http://cloud.simssa.ca/index.php/s/JuPGmwlvAP9ckkR},
	booktitle = {Proceedings of the {International} {Society} for {Music} {Information} {Retrieval}},
	author = {Calvo-Zaragoza, Jorge and Vigliensoni, Gabriel and Fujinaga, Ichiro},
	year = {2017},
	pages = {724--30}
}

@inproceedings{vigliensoni_music_2017,
	address = {Suzhou, China},
	title = {The {Music} {Listening} {Histories} {Dataset}},
	url = {http://cloud.simssa.ca/index.php/s/HUvFrpFl0ErRVz9},
	booktitle = {Proceedings of the {International} {Society} for {Music} {Information} {Retrieva}},
	author = {Vigliensoni, Gabriel and Fujinaga, Ichiro},
	year = {2017},
	pages = {96--102}
}

@inproceedings{vigliensoni_developing_2018,
	address = {Paris, France},
	title = {Developing an environment for teaching computers to read music},
	url = {http://cloud.simssa.ca/index.php/s/ImKlwsuLoI099uI},
	booktitle = {Proceedings of 1st {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {Vigliensoni, Gabriel and Calvo-Zaragoza, Jorge and Fujinaga, Ichiro},
	year = {2018}
}

@inproceedings{castellanos_document_2018,
	address = {Paris, France},
	title = {Document analysis of music score images with selectional auto encoders},
	url = {http://cloud.simssa.ca/index.php/s/FjJGQ6josKEIWNn},
	booktitle = {Proceedings of the 19th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Castellanos, Francisco and Calvo-Zaragoza, Jorge and Vigliensoni, Gabriel and Fujinaga, Ichiro},
	year = {2018}
}

@inproceedings{napoles_lopez_encoding_2018,
	address = {Paris, France},
	title = {Encoding {Matters}},
	url = {http://cloud.simssa.ca/index.php/s/lofhdmzzF9VKQ4m},
	booktitle = {Proceedings of the 5th {International} {Conference} on {Digital} {Libraries} for {Musicology}},
	author = {Nápoles López, Néstor and Vigliensoni, Gabriel and Fujinaga, Ichiro},
	year = {2018}
}

@inproceedings{cumming_methodologies_2018,
	address = {Paris, France},
	title = {Methodologies for creating symbolic corpora of {Western} music before 1600},
	booktitle = {Proceedings of the 19th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	publisher = {ISMIR},
	author = {Cumming, Julie E. and McKay, Cory and Stuchbery, Jonathan and Fujinaga, Ichiro},
	year = {2018},
	pages = {491--8}
}

@inproceedings{arthur_role_2018-1,
	address = {Montreal, QC},
	title = {The {Role} of {Structural} {Tones} in {Establishing} {Mode} in {Renaissance} {Two}-part {Counterpoint}},
	booktitle = {Proceedings of the 15th {International} {Conference} on {Music} {Perception} and {Cognition}},
	author = {Arthur, Claire and Cumming, Julie and Schubert, Peter},
	year = {2018}
}

@article{de_luca_capturing_nodate,
	title = {Capturing {Early} {Notations} in {MEI}: {The} {Case} of {Old} {Hispanic} {Neumes}},
	volume = {2},
	journal = {Musiktheorie-Zeitschrift für Musikwissenschaft},
	author = {De Luca, Elsa and Behrendt, Inga and Fujinaga, Ichiro and Helsen, Kate and Ignesti, Alessandra and Lacoste, Debra and Long, Sarah Ann}
}

@inproceedings{fujinaga_art_2019,
	title = {The {Art} of {Teaching} {Computers}: {The} {SIMSSA} {Optical} {Music} {Recognition} {Workflow} {System}},
	url = {http://cloud.simssa.ca/index.php/s/K2PwiZlzrdpHSOq},
	doi = {10.23919/EUSIPCO.2019.8902658},
	booktitle = {Proceedings of the 27th {European} {Signal} {Processing} {Conference}},
	author = {Fujinaga, Ichiro and Vigliensoni, Gabriel},
	year = {2019}
}

@incollection{cumming_contrapuntal_2020,
	title = {Contrapuntal {Style}: {Josquin} vs. {La} {Rue}},
	booktitle = {La {Rue} {Studies}},
	author = {Cumming, Julie and McKay, Cory and Schubert, Peter and Nápoles López, Néstor and Margot, Sylvain},
	editor = {Burn, David and Meconi, Honey},
	year = {2020}
}

@article{cumming_why_2019,
	title = {Why {Should} {Musicologists} do {Digital} {Humanities}?},
	journal = {Troja: Jahrbuch für Renaissancemusik},
	author = {Cumming, Julie},
	year = {2019}
}

@incollection{cumming_edinburgh_2019,
	address = {Edinburgh, Scotland},
	title = {The {Edinburgh} {History} of {Distributed} {Cognition}: {Distributed} {Cognition}, {Improvisation}, and the {Arts} in {Early} {Modern} {Europe}},
	volume = {2},
	booktitle = {Distributed {Cognition} in {Medieval} and {Renaissance} {Culture}},
	publisher = {Edinburgh University Press},
	author = {Cumming, Julie and Tribble, Evelyn},
	editor = {Anderson, Miranda and Wheeler, Michael},
	year = {2019}
}

@article{cumming_du_2017,
	title = {Du {Fay}’s {Use} of {Improvisatory} {Techniques} in {Resvellies} vous et faites chiere lye},
	volume = {17},
	number = {2},
	journal = {Composition and Improvisation in Fifteenth-Century Music, a special issue of the RATM (Rivista di Analisi e Teoria Musicale)},
	author = {Cumming, Julie},
	editor = {Cumming, Julie and Rodin, Jesse and Locanto, Massimiliano},
	year = {2017},
	pages = {3--23}
}

@inproceedings{napoles_lopez_key-finding_nodate,
	address = {The Hague, Netherlands},
	title = {Key-{Finding} {Algorithm} {Based} on a {Hidden} {Markov} {Model} and {Key} {Profiles}},
	booktitle = {Proceedings of the 6th {International} {Conference} on {Digital} {Libraries} for {Musicology} ({DLfM}19)},
	author = {Nápoles López, Néstor and Arthur, Claire and Fujinaga, Ichiro}
}

@inproceedings{shaw_differentiae_2018,
	title = {Differentiae in the {Cantus} {Manuscript} {Database}},
	booktitle = {Proceedings of the 6th {International} {Conference} on {Digital} {Libraries} for {Musicology} ({DLfM}19)},
	author = {Shaw, Rebecca},
	year = {2018}
}

@incollection{cumming_sources_2017,
	address = {Turnhout, Belgium},
	title = {Sources and {Identity}: {Composers} and {Singers} in {Darnton}’s {Communications} {Circuit}},
	booktitle = {Sources of {Identity}: {Makers}, {Owners} and {Users} of {Music} {Sources} {Before} 1600},
	publisher = {Brepols},
	author = {Cumming, Julie},
	editor = {Shepherd, Tim and Colton, Lisa},
	year = {2017}
}

@inproceedings{pugin_verovio:_2014-1,
	address = {University of Virginia, Charlottesville, VA},
	title = {Verovio: a {Library} for {Typesetting} {MEI}},
	booktitle = {Proceedings of the {Music} {Encoding} {Conference} (2013-2014)},
	publisher = {Music Encoding Initiative},
	author = {Pugin, Laurent and Zitellini, Rodolfo},
	year = {2014}
}

@inproceedings{di_bacco_mei_2015,
	address = {Florence, Italy},
	title = {{MEI} for {Mensural} {Notation} in the {Thesaurus} {Musicarum} {Latinarum}},
	booktitle = {Proceedings of the {Music} {Encoding} {Conference} (2015-2017)},
	publisher = {Music Encoding Initiative},
	author = {Di Bacco, Giuliano and Roland, Perry},
	year = {2015},
	pages = {25--36}
}

@inproceedings{lewis_capturing_2017,
	address = {Tours, France},
	title = {Capturing {Context} and {Provenance} of {Musicology} {Research}},
	booktitle = {Proceedings of the {Music} {Encoding} {Conference} (2015-2017)},
	publisher = {Music Encoding Initiative},
	author = {Lewis, David and Page, Kevin and Hankinson, Andrew},
	year = {2017},
	pages = {113--118}
}

@article{calvo-zaragoza_selectional_2019,
	title = {A selectional auto-encoder approach for document image binarization},
	volume = {86},
	issn = {0031-3203},
	url = {http://www.sciencedirect.com/science/article/pii/S0031320318303091},
	doi = {10.1016/j.patcog.2018.08.011},
	abstract = {Binarization plays a key role in the automatic information retrieval from document images. This process is usually performed in the first stages of document analysis systems, and serves as a basis for subsequent steps. Hence it has to be robust in order to allow the full analysis workflow to be successful. Several methods for document image binarization have been proposed so far, most of which are based on hand-crafted image processing strategies. Recently, Convolutional Neural Networks have shown an amazing performance in many disparate duties related to computer vision. In this paper we discuss the use of convolutional auto-encoders devoted to learning an end-to-end map from an input image to its selectional output, in which activations indicate the likelihood of pixels to be either foreground or background. Once trained, documents can therefore be binarized by parsing them through the model and applying a global threshold. This approach has proven to outperform existing binarization strategies in a number of document types.},
	urldate = {2019-10-01},
	journal = {Pattern Recognition},
	author = {Calvo-Zaragoza, Jorge and Gallego, Antonio-Javier},
	month = feb,
	year = {2019},
	keywords = {Auto-encoders, Binarization, Convolutional Neural Networks, Document analysis},
	pages = {37--47},
	file = {ScienceDirect Full Text PDF:/Users/emilyhopkins/Zotero/storage/P4QVY5AD/Calvo-Zaragoza and Gallego - 2019 - A selectional auto-encoder approach for document i.pdf:application/pdf;ScienceDirect Snapshot:/Users/emilyhopkins/Zotero/storage/5XIBFP5K/S0031320318303091.html:text/html}
}

@article{baro_optical_2019,
	title = {From {Optical} {Music} {Recognition} to {Handwritten} {Music} {Recognition}: {A} baseline},
	volume = {123},
	issn = {0167-8655},
	shorttitle = {From {Optical} {Music} {Recognition} to {Handwritten} {Music} {Recognition}},
	url = {http://www.sciencedirect.com/science/article/pii/S0167865518303386},
	doi = {10.1016/j.patrec.2019.02.029},
	abstract = {Optical Music Recognition (OMR) is the branch of document image analysis that aims to convert images of musical scores into a computer-readable format. Despite decades of research, the recognition of handwritten music scores, concretely the Western notation, is still an open problem, and the few existing works only focus on a specific stage of OMR. In this work, we propose a full Handwritten Music Recognition (HMR) system based on Convolutional Recurrent Neural Networks, data augmentation and transfer learning, that can serve as a baseline for the research community.},
	urldate = {2019-10-01},
	journal = {Pattern Recognition Letters},
	author = {Baró, Arnau and Riba, Pau and Calvo-Zaragoza, Jorge and Fornés, Alicia},
	month = may,
	year = {2019},
	keywords = {Deep neural networks, Document image analysis and recognition, Handwritten music recognition, LSTM, Optical music recognition},
	pages = {1--8},
	file = {ScienceDirect Full Text PDF:/Users/emilyhopkins/Zotero/storage/MWCVEHK3/Baró et al. - 2019 - From Optical Music Recognition to Handwritten Musi.pdf:application/pdf;ScienceDirect Snapshot:/Users/emilyhopkins/Zotero/storage/FDTRWACR/S0167865518303386.html:text/html}
}

@inproceedings{mateiu_domain_2019,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Domain {Adaptation} for {Handwritten} {Symbol} {Recognition}: {A} {Case} of {Study} in {Old} {Music} {Manuscripts}},
	isbn = {978-3-030-31321-0},
	shorttitle = {Domain {Adaptation} for {Handwritten} {Symbol} {Recognition}},
	language = {en},
	booktitle = {Pattern {Recognition} and {Image} {Analysis}},
	publisher = {Springer International Publishing},
	author = {Mateiu, Tudor N. and Gallego, Antonio-Javier and Calvo-Zaragoza, Jorge},
	editor = {Morales, Aythami and Fierrez, Julian and Sánchez, José Salvador and Ribeiro, Bernardete},
	year = {2019},
	keywords = {Convolutional Neural Network, Domain Adaptation, Handwritten music symbols, Optical Music Recognition},
	pages = {135--146},
	file = {Springer Full Text PDF:/Users/emilyhopkins/Zotero/storage/KVUE9R7F/Mateiu et al. - 2019 - Domain Adaptation for Handwritten Symbol Recogniti.pdf:application/pdf}
}

@inproceedings{alfaro-contreras_approaching_2019,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Approaching {End}-to-{End} {Optical} {Music} {Recognition} for {Homophonic} {Scores}},
	isbn = {978-3-030-31321-0},
	abstract = {The recognition of patterns that have a time dependency is common in areas like speech recognition or natural language processing. The equivalent situation in image analysis is present in tasks like text or video recognition. Recently, Recurrent Neural Networks (RNN) have been broadly applied to solve these task with good results in an end-to-end fashion. However, its application to Optical Music Recognition (OMR) is not so straightforward due to the presence of different elements at the same horizontal position, disrupting the linear flow of the time line. In this paper we study the ability of the RNNs to learn codes that represent this disruption in homophonic scores. The results prove that our serialized ways of encoding the music content are appropriate for Deep Learning-based OMR and they deserve further study.},
	language = {en},
	booktitle = {Pattern {Recognition} and {Image} {Analysis}},
	publisher = {Springer International Publishing},
	author = {Alfaro-Contreras, María and Calvo-Zaragoza, Jorge and Iñesta, José M.},
	editor = {Morales, Aythami and Fierrez, Julian and Sánchez, José Salvador and Ribeiro, Bernardete},
	year = {2019},
	keywords = {Optical Music Recognition, Deep Learning, End-to-end recognition, Music encoding},
	pages = {147--158},
	file = {Springer Full Text PDF:/Users/emilyhopkins/Zotero/storage/U9UQMSJ8/Alfaro-Contreras et al. - 2019 - Approaching End-to-End Optical Music Recognition f.pdf:application/pdf}
}

@inproceedings{nunez-alcover_glyph_2019,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Glyph and {Position} {Classification} of {Music} {Symbols} in {Early} {Music} {Manuscripts}},
	isbn = {978-3-030-31321-0},
	abstract = {Optical Music Recognition is a field of research that automates the reading of musical scores so as to transcribe their content into a structured digital format. When dealing with music manuscripts, the traditional workflow establishes separate stages of detection and classification of musical symbols. In the latter, most of the research has focused on detecting musical glyphs, ignoring that the meaning of a musical symbol is defined by two components: its glyph and its position within the staff. In this paper we study how to perform both glyph and position classification of handwritten musical symbols in early music manuscripts written in white Mensural notation, a common notation system used for the most part of the XVI and XVII centuries. We make use of Convolutional Neural Networks as the classification method, and we tested several alternatives such as using independent models for each component, combining label spaces, or using both multi-input and multi-output models. Our results on early music manuscripts provide insights about the effectiveness and efficiency of each approach.},
	language = {en},
	booktitle = {Pattern {Recognition} and {Image} {Analysis}},
	publisher = {Springer International Publishing},
	author = {Nuñez-Alcover, Alicia and de León, Pedro J. Ponce and Calvo-Zaragoza, Jorge},
	editor = {Morales, Aythami and Fierrez, Julian and Sánchez, José Salvador and Ribeiro, Bernardete},
	year = {2019},
	keywords = {Convolutional Neural Networks, Optical Music Recognition, Digital preservation, Handwritten symbol classification},
	pages = {159--168},
	file = {Springer Full Text PDF:/Users/emilyhopkins/Zotero/storage/L5XIFBYT/Nuñez-Alcover et al. - 2019 - Glyph and Position Classification of Music Symbols.pdf:application/pdf}
}

@inproceedings{rizo_muret:_2018,
	address = {New York, NY, USA},
	series = {{DLfM} '18},
	title = {{MuRET}: {A} {Music} {Recognition}, {Encoding}, and {Transcription} {Tool}},
	isbn = {978-1-4503-6522-2},
	shorttitle = {{MuRET}},
	url = {http://doi.acm.org/10.1145/3273024.3273029},
	doi = {10.1145/3273024.3273029},
	abstract = {The transcription process from early and modern notation manuscripts to a structured digital encoding has been traditionally performed following a fully manual workflow. At most it has received some technological support in particular stages, like optical music recognition (OMR) of the source images, or transcription to modern notation with music edition applications. Currently, there is no mature and stable enough solution for the OMR problem, and the most used music editors do not support early notations, such as the mensural one. In this work, a new tool called MUsic Recognition, Encoding, and Transcription (MuRET) is introduced, which covers all transcription phases, from the manuscript source to the encoded digital content. MuRET is designed as a technology-focused research tool, allowing different processing approaches to be used, and producing both the expected transcribed contents in standard encodings and data for the study of the transcription process itself.},
	urldate = {2019-10-01},
	booktitle = {Proceedings of the 5th {International} {Conference} on {Digital} {Libraries} for {Musicology}},
	publisher = {ACM},
	author = {Rizo, David and Calvo-Zaragoza, Jorge and Iñesta, José M.},
	year = {2018},
	note = {event-place: Paris, France},
	keywords = {encoding, notation transcription, optical music recognition},
	pages = {52--56},
	file = {ACM Full Text PDF:/Users/emilyhopkins/Zotero/storage/9TZMHXKM/Rizo et al. - 2018 - MuRET A Music Recognition, Encoding, and Transcri.pdf:application/pdf}
}

@inproceedings{hajic_jr._how_2018,
	address = {New York, NY, USA},
	series = {{DLfM} '18},
	title = {How {Current} {Optical} {Music} {Recognition} {Systems} {Are} {Becoming} {Useful} for {Digital} {Libraries}},
	isbn = {978-1-4503-6522-2},
	url = {http://doi.acm.org/10.1145/3273024.3273034},
	doi = {10.1145/3273024.3273034},
	abstract = {Optical Music Recognition (OMR) promises to make large collections of sheet music searchable by their musical content. It would open up novel ways of accessing the vast amount of written music that has never been recorded before. For a long time, OMR was not living up to that promise, as its performance was simply not good enough, especially on handwritten music or under non-ideal image conditions. However, OMR has recently seen a number of improvements, mainly due to the advances in machine learning. In this work, we take an OMR system based on the traditional pipeline and an end-to-end system, which represent the current state of the art, and illustrate in proof-of-concept experiments their applicability in retrieval settings. We also provide an example of a musicological study that can be replicated with OMR outputs at much lower costs. Taken together, this indicates that in some settings, current OMR can be used as a general tool for enriching digital libraries.},
	urldate = {2019-10-01},
	booktitle = {Proceedings of the 5th {International} {Conference} on {Digital} {Libraries} for {Musicology}},
	publisher = {ACM},
	author = {Hajič Jr., Jan and Kolárová, Marta and Pacha, Alexander and Calvo-Zaragoza, Jorge},
	year = {2018},
	note = {event-place: Paris, France},
	keywords = {optical music recognition, digital musicology, music digital libraries, music information retrieval, symbolic music search},
	pages = {57--61},
	file = {ACM Full Text PDF:/Users/emilyhopkins/Zotero/storage/CVVQJ98S/jr et al. - 2018 - How Current Optical Music Recognition Systems Are .pdf:application/pdf}
}

@inproceedings{calvo-zaragoza_probabilistic_2018,
	title = {Probabilistic {Music}-{Symbol} {Spotting} in {Handwritten} {Scores}},
	doi = {10.1109/ICFHR-2018.2018.00103},
	abstract = {Content-based search on musical manuscripts is usually performed assuming that there are accurate transcripts of the sources in a symbolic, structured format. Given that current systems for Handwritten Music Recognition are far from offering guarantees about their accuracy, this traditional approach does not represent a scalable scenario. In this work we propose a probabilistic framework for Music-Symbol Spotting (MSS), that allows for content-based music search directly over the images of the manuscripts. By means of statistical recognition systems, a probabilistic index is built upon which the search can be carried out efficiently. Our experiments over a dataset of an Early handwritten music manuscript in Mensural notation demonstrates that this MSS framework can be presented as a promising alternative to the traditional approach for content-based music search.},
	booktitle = {2018 16th {International} {Conference} on {Frontiers} in {Handwriting} {Recognition} ({ICFHR})},
	author = {Calvo-Zaragoza, J. and Toselli, A. H. and Vidal, E.},
	month = aug,
	year = {2018},
	keywords = {accurate transcripts, content-based music search, early handwritten music manuscript, Handwriting recognition, handwritten character recognition, handwritten music recognition, handwritten scores, Hidden Markov models, Indexes, information retrieval, MSS framework, music, Music, Music Spotting, Music manuscripts, Hidden Markov Models, Probabilistic indexing, Mensural Notation, musical manuscripts, probabilistic framework, probabilistic index, Probabilistic logic, probabilistic music-symbol spotting, probability, Random variables, statistical recognition systems, structured format, symbolic format, Task analysis, traditional approach},
	pages = {558--563},
	file = {IEEE Xplore Abstract Record:/Users/emilyhopkins/Zotero/storage/IUGNAQDD/8583821.html:text/html;IEEE Xplore Full Text PDF:/Users/emilyhopkins/Zotero/storage/89WZ5N99/Calvo-Zaragoza et al. - 2018 - Probabilistic Music-Symbol Spotting in Handwritten.pdf:application/pdf}
}

@inproceedings{calvo-zaragoza_camera-primus:_2018,
	address = {Paris},
	title = {Camera-{PrIMus}: {Neural} {End}-to-{End} {Optical} {Music} {Recognition} on {Realistic} {Monophonic} {Scores}},
	abstract = {The optical music recognition (OMR) ﬁeld studies how to automate the process of reading the musical notation present in a given image. Among its many uses, an interesting scenario is that in which a score captured with a camera is to be automatically reproduced. Recent approaches to OMR have shown that the use of deep neural networks allows important advances in the ﬁeld. However, these approaches have been evaluated on images with ideal conditions, which do not correspond to the previous scenario. In this work, we evaluate the performance of an end-to-end approach that uses a deep convolutional recurrent neural network (CRNN) over non-ideal image conditions of music scores. Consequently, our contribution also consists of Camera-PrIMuS, a corpus of printed monophonic scores of real music synthetically modiﬁed to resemble camera-based realistic scenarios, involving distortions such as irregular lighting, rotations, or blurring. Our results conﬁrm that the CRNN is able to successfully solve the task under these conditions, obtaining an error around 2\% at music-symbol level, thereby representing a groundbreaking piece of research towards useful OMR systems.},
	language = {en},
	booktitle = {Proceedings of the 19th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Calvo-Zaragoza, Jorge and Rizo, David},
	year = {2018},
	pages = {8},
	file = {Calvo-Zaragoza and Rizo - 2018 - CAMERA-PRIMUS NEURAL END-TO-END OPTICAL MUSIC REC.pdf:/Users/emilyhopkins/Zotero/storage/2XQZ8BHM/Calvo-Zaragoza and Rizo - 2018 - CAMERA-PRIMUS NEURAL END-TO-END OPTICAL MUSIC REC.pdf:application/pdf}
}

@article{calvo-zaragoza_recognition_2017,
	title = {Recognition of pen-based music notation with finite-state machines},
	volume = {72},
	issn = {0957-4174},
	url = {http://www.sciencedirect.com/science/article/pii/S0957417416305838},
	doi = {10.1016/j.eswa.2016.10.041},
	abstract = {This work presents a statistical model to recognize pen-based music compositions using stroke recognition algorithms and finite-state machines. The series of strokes received as input is mapped onto a stochastic representation, which is combined with a formal language that describes musical symbols in terms of stroke primitives. Then, a Probabilistic Finite-State Automaton is obtained, which defines probabilities over the set of musical sequences. This model is eventually crossed with a semantic language to avoid sequences that does not make musical sense. Finally, a decoding strategy is applied in order to output a hypothesis about the musical sequence actually written. Comprehensive experimentation with several decoding algorithms, stroke similarity measures and probability density estimators are tested and evaluated following different metrics of interest. Results found have shown the goodness of the proposed model, obtaining competitive performances in all metrics and scenarios considered.},
	urldate = {2019-10-01},
	journal = {Expert Systems with Applications},
	author = {Calvo-Zaragoza, Jorge and Oncina, Jose},
	month = apr,
	year = {2017},
	keywords = {Optical music recognition, Finite-State machines, Pen-based recognition},
	pages = {395--406},
	file = {ScienceDirect Full Text PDF:/Users/emilyhopkins/Zotero/storage/PBJ9NB9Y/Calvo-Zaragoza and Oncina - 2017 - Recognition of pen-based music notation with finit.pdf:application/pdf;ScienceDirect Snapshot:/Users/emilyhopkins/Zotero/storage/PYG872K5/S0957417416305838.html:text/html}
}

@article{gallego_staff-line_2017,
	title = {Staff-line removal with selectional auto-encoders},
	volume = {89},
	issn = {0957-4174},
	url = {http://www.sciencedirect.com/science/article/pii/S0957417417304712},
	doi = {10.1016/j.eswa.2017.07.002},
	abstract = {Staff-line removal is an important preprocessing stage as regards most Optical Music Recognition systems. The common procedures employed to carry out this task involve image processing techniques. In contrast to these traditional methods, which are based on hand-engineered transformations, the problem can also be approached from a machine learning point of view if representative examples of the task are provided. We propose doing this through the use of a new approach involving auto-encoders, which select the appropriate features of an input feature set (Selectional Auto-Encoders). Within the context of the problem at hand, the model is trained to select those pixels of a given image that belong to a musical symbol, thus removing the lines of the staves. Our results show that the proposed technique is quite competitive and significantly outperforms the other state-of-art strategies considered, particularly when dealing with grayscale input images.},
	urldate = {2019-10-01},
	journal = {Expert Systems with Applications},
	author = {Gallego, Antonio-Javier and Calvo-Zaragoza, Jorge},
	month = dec,
	year = {2017},
	keywords = {Auto-encoders, Optical music recognition, Convolutional networks, Staff-line removal},
	pages = {138--148},
	file = {ScienceDirect Full Text PDF:/Users/emilyhopkins/Zotero/storage/UGBB5KLV/Gallego and Calvo-Zaragoza - 2017 - Staff-line removal with selectional auto-encoders.pdf:application/pdf;ScienceDirect Snapshot:/Users/emilyhopkins/Zotero/storage/9XGM33BV/S0957417417304712.html:text/html}
}

@article{calvo-zaragoza_staff-line_2017,
	title = {Staff-line detection and removal using a convolutional neural network},
	volume = {28},
	issn = {1432-1769},
	url = {https://doi.org/10.1007/s00138-017-0844-4},
	doi = {10.1007/s00138-017-0844-4},
	abstract = {Staff-line removal is an important preprocessing stage for most optical music recognition systems. Common procedures to solve this task involve image processing techniques. In contrast to these traditional methods based on hand-engineered transformations, the problem can also be approached as a classification task in which each pixel is labeled as either staff or symbol, so that only those that belong to symbols are kept in the image. In order to perform this classification, we propose the use of convolutional neural networks, which have demonstrated an outstanding performance in image retrieval tasks. The initial features of each pixel consist of a square patch from the input image centered at that pixel. The proposed network is trained by using a dataset which contains pairs of scores with and without the staff lines. Our results in both binary and grayscale images show that the proposed technique is very accurate, outperforming both other classifiers and the state-of-the-art strategies considered. In addition, several advantages of the presented methodology with respect to traditional procedures proposed so far are discussed.},
	language = {en},
	number = {5},
	urldate = {2019-10-01},
	journal = {Machine Vision and Applications},
	author = {Calvo-Zaragoza, Jorge and Pertusa, Antonio and Oncina, Jose},
	month = aug,
	year = {2017},
	keywords = {Optical music recognition, Convolutional neural networks, Music staff-line removal, Pixel classification},
	pages = {665--674},
	file = {Springer Full Text PDF:/Users/emilyhopkins/Zotero/storage/AMRJMDXQ/Calvo-Zaragoza et al. - 2017 - Staff-line detection and removal using a convoluti.pdf:application/pdf}
}

@inproceedings{sober-mira_pen-based_2018,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Pen-{Based} {Music} {Document} {Transcription} with {Convolutional} {Neural} {Networks}},
	isbn = {978-3-030-02284-6},
	abstract = {The transcription of music sources requires new ways of interacting with musical documents. Assuming that automatic technologies will never guarantee a perfect transcription, our intention is to develop an interactive system in which user and software collaborate to complete the task. Since the use of traditional software for score edition might be tedious, our work studies the interaction by means of electronic pen (e-pen). In our framework, users trace symbols using an e-pen over a digital surface, which provides both the underlying image (offline data) and the drawing made (online data). Using both sources, the system is capable of reaching an error below 4\% when recognizing the symbols with a Convolutional Neural Network.},
	language = {en},
	booktitle = {2017 14th {IAPR} {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	publisher = {Springer International Publishing},
	author = {Sober-Mira, Javier and Calvo-Zaragoza, Jorge and Rizo, David and Iñesta, José M.},
	editor = {Fornés, Alicia and Lamiroy, Bart},
	year = {2018},
	keywords = {Convolutional Neural Networks, Optical music recognition, Music documents, Pen-based technologies},
	pages = {71--80},
	file = {Springer Full Text PDF:/Users/emilyhopkins/Zotero/storage/X6XCFD2F/Sober-Mira et al. - 2018 - Pen-Based Music Document Transcription with Convol.pdf:application/pdf}
}

@inproceedings{baro_optical_2018,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Optical {Music} {Recognition} by {Long} {Short}-{Term} {Memory} {Networks}},
	isbn = {978-3-030-02284-6},
	abstract = {Optical Music Recognition refers to the task of transcribing the image of a music score into a machine-readable format. Many music scores are written in a single staff, and therefore, they could be treated as a sequence. Therefore, this work explores the use of Long Short-Term Memory (LSTM) Recurrent Neural Networks for reading the music score sequentially, where the LSTM helps in keeping the context. For training, we have used a synthetic dataset of more than 40000 images, labeled at primitive level. The experimental results are promising, showing the benefits of our approach.},
	language = {en},
	booktitle = {{GREC} 2017: {Graphics} {Recognition}. {Current} {Trends} and {Evolutions}},
	publisher = {Springer International Publishing},
	author = {Baró, Arnau and Riba, Pau and Calvo-Zaragoza, Jorge and Fornés, Alicia},
	editor = {Fornés, Alicia and Lamiroy, Bart},
	year = {2018},
	keywords = {Optical music recognition, Long short-term memory, Recurrent neural network},
	pages = {81--95},
	file = {Springer Full Text PDF:/Users/emilyhopkins/Zotero/storage/5F46JUTP/Baró et al. - 2018 - Optical Music Recognition by Long Short-Term Memor.pdf:application/pdf}
}

@inproceedings{calvo-zaragoza_discussion_2018,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Discussion {Group} {Summary}: {Optical} {Music} {Recognition}},
	isbn = {978-3-030-02284-6},
	shorttitle = {Discussion {Group} {Summary}},
	abstract = {This document summarizes the discussion of the interest group on Optical Music Recognition (OMR) that took place in the 12th IAPR International Workshop on Graphics Recognition, and presents the main conclusions drawn during the session: OMR should revisit how it describes itself, and the OMR community should intensify its collaboration both internally and with other stakeholders.},
	language = {en},
	booktitle = {{GREC} 2017: {Graphics} {Recognition}. {Current} {Trends} and {Evolutions}},
	publisher = {Springer International Publishing},
	author = {Calvo-Zaragoza, Jorge and Hajič, Jan and Pacha, Alexander},
	editor = {Fornés, Alicia and Lamiroy, Bart},
	year = {2018},
	keywords = {Optical Music Recognition, Discussion group},
	pages = {152--157},
	file = {Springer Full Text PDF:/Users/emilyhopkins/Zotero/storage/26AUXHYH/Calvo-Zaragoza et al. - 2018 - Discussion Group Summary Optical Music Recognitio.pdf:application/pdf}
}

@inproceedings{calvo-zaragoza_staff-line_2017-1,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Staff-{Line} {Detection} on {Grayscale} {Images} with {Pixel} {Classification}},
	isbn = {978-3-319-58838-4},
	url = {http://cloud.simssa.ca/index.php/s/tZswNa5gjwkoatf},
	abstract = {Staff-line detection and removal are important processing steps in most Optical Music Recognition systems. Traditional methods make use of heuristic strategies based on image processing techniques with binary images. However, binarization is a complex process for which it is difficult to achieve perfect results. In this paper we describe a novel staff-line detection and removal method that deals with grayscale images directly. Our approach uses supervised learning to classify each pixel of the image as symbol, staff, or background. This classification is achieved by means of Convolutional Neural Networks. The features of each pixel consist of a square window from the input image centered at the pixel to be classified. As a case of study, we performed experiments with the CVC-Muscima dataset. Our approach showed promising performance, outperforming state-of-the-art algorithms for staff-line removal.},
	language = {en},
	booktitle = {Pattern {Recognition} and {Image} {Analysis}},
	publisher = {Springer International Publishing},
	author = {Calvo-Zaragoza, Jorge and Vigliensoni, Gabriel and Fujinaga, Ichiro},
	editor = {Alexandre, Luís A. and Salvador Sánchez, José and Rodrigues, João M. F.},
	year = {2017},
	keywords = {Grayscale domain, Music recognition, Staff-line removal},
	pages = {279--286},
	file = {Springer Full Text PDF:/Users/emilyhopkins/Zotero/storage/JLUQWLDY/Calvo-Zaragoza et al. - 2017 - Staff-Line Detection on Grayscale Images with Pixe.pdf:application/pdf}
}

@inproceedings{calvo-zaragoza_music_2017,
	address = {Tokyo, Japan},
	title = {Music {Document} {Layout} {Analysis} through {Machine} {Learning} and {Human} {Feedback}},
	volume = {02},
	url = {http://cloud.simssa.ca/index.php/s/aT7YRypcBfT85tA},
	doi = {10.1109/ICDAR.2017.259},
	abstract = {Music documents often include musical symbols as well as other relevant elements such as staff lines, text, and decorations. To detect and separate these constituent elements, we propose a layout analysis framework based on machine learning that focuses on pixel-level classification of the image. For that, we make use of supervised learning classifiers trained to infer the category of each pixel. In addition, our scenario considers a human-aided computing approach in which the user is part of the recognition loop, providing feedback where relevant errors are made.},
	booktitle = {2017 14th {IAPR} {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	author = {Calvo-Zaragoza, Jorge and Zhang, Ké and Saleh, Zeyad and Vigliensoni, Gabriel and Fujinaga, Ichiro},
	month = nov,
	year = {2017},
	keywords = {Algorithm design and analysis, document image processing, feedback, human feedback, Human-aided computing, human-aided computing approach, image classification, image recognition, Layout, layout analysis framework, learning (artificial intelligence), machine learning, Machine Learning, Machine learning algorithms, Multiple signal classification, music, Music, music document layout analysis, Music Document Layout Analysis, music documents, musical symbols, Optical Music Recognition, pixel-level image classification, staff lines, supervised learning classifiers, Task analysis, Text analysis},
	pages = {23--24},
	file = {IEEE Xplore Abstract Record:/Users/emilyhopkins/Zotero/storage/MGU79I4Y/8270201.html:text/html;IEEE Xplore Full Text PDF:/Users/emilyhopkins/Zotero/storage/LHZ3SVSU/Calvo-Zaragoza et al. - 2017 - Music Document Layout Analysis through Machine Lea.pdf:application/pdf}
}

@inproceedings{calvo-zaragoza_recognition_2017-1,
	title = {Recognition of {Handwritten} {Music} {Symbols} with {Convolutional} {Neural} {Codes}},
	volume = {01},
	doi = {10.1109/ICDAR.2017.118},
	abstract = {There are large collections of music manuscripts preserved over the centuries. In order to analyze these documents it is necessary to transcribe them into a machine-readable format. This process can be done automatically using Optical Music Recognition (OMR) systems, which typically consider segmentation plus classification workflows. This work is focused on the latter stage, presenting a comprehensive study for classification of handwritten musical symbols using Convolutional Neural Networks (CNN). The power of these models lies in their ability to transform the input into a meaningful representation for the task at hand, and that is why we study the use of these models to extract features (Neural Codes) for other classifiers. For the evaluation we consider four datasets containing different configurations and notation styles, along with a number of network models, different image preprocessing techniques and several supervised learning classifiers. Our results show that a remarkable accuracy can be achieved using the proposed framework, which significantly outperforms the state of the art in all datasets considered.},
	booktitle = {2017 14th {IAPR} {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	author = {Calvo-Zaragoza, Jorge and Gallego, Antonio-Javier and Pertusa, Antonio},
	month = nov,
	year = {2017},
	keywords = {Convolutional Neural Networks, Optical Music Recognition, handwritten character recognition, music, Music, Task analysis, Convolutional neural networks, image classification, learning (artificial intelligence), convolution, Convolutional codes, feature extraction, Feature extraction, Handwritten Music Symbols, handwritten musical symbols, image representation, image segmentation, machine-readable format, music manuscripts, Neural Codes, neural nets, optical character recognition, Optical Music Recognition systems, Support vector machines, Training},
	pages = {691--696},
	file = {IEEE Xplore Abstract Record:/Users/emilyhopkins/Zotero/storage/PVXTLR9F/8270049.html:text/html;IEEE Xplore Full Text PDF:/Users/emilyhopkins/Zotero/storage/XW67LDL3/Calvo-Zaragoza et al. - 2017 - Recognition of Handwritten Music Symbols with Conv.pdf:application/pdf}
}

@inproceedings{calvo-zaragoza_handwritten_2017,
	title = {Handwritten {Music} {Recognition} for {Mensural} {Notation}: {Formulation}, {Data} and {Baseline} {Results}},
	volume = {01},
	shorttitle = {Handwritten {Music} {Recognition} for {Mensural} {Notation}},
	doi = {10.1109/ICDAR.2017.179},
	abstract = {Music is a key element for cultural transmission, and so large collections of music manuscripts have been preserved over the centuries. In order to develop computational tools for analysis, indexing and retrieval from these sources, it is necessary to transcribe the content to some machine-readable format. In this paper we discuss the Handwritten Music Recognition problem, which refers to the development of automatic transcription systems for musical manuscripts. We focus on mensural notation, one of the most widespread varieties of Western classical music. For that, we present a labeled corpus containing 576 staves, along with a baseline recognition system based on a combination of hidden Markov models and N-gram language models. The baseline error obtained at symbol level is about 40 \% which, given the difficulty of the task, can be considered a good starting point for future developments. Our aim is that these data and preliminary results help to promote this research field, serving as a reference in future developments.},
	booktitle = {2017 14th {IAPR} {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	author = {Calvo-Zaragoza, J. and Toselli, A. H. and Vidal, E.},
	month = nov,
	year = {2017},
	keywords = {Optical Music Recognition, Handwriting recognition, Hidden Markov models, music, Music, musical manuscripts, Task analysis, machine-readable format, automatic transcription systems, baseline error, baseline recognition system, computational tools, cultural transmission, Handwritten Music Recognition, handwritten music recognition problem, hidden Markov models, Hidden Markov Models, Image segmentation, indexing, labeled corpus, mensural notation, Mensural notation, n-gram language models, Natural languages, retrieval, Shape, speech recognition, Western classical music},
	pages = {1081--1086},
	file = {IEEE Xplore Abstract Record:/Users/emilyhopkins/Zotero/storage/R9FY6I8S/8270110.html:text/html;IEEE Xplore Full Text PDF:/Users/emilyhopkins/Zotero/storage/F48GVHFU/Calvo-Zaragoza et al. - 2017 - Handwritten Music Recognition for Mensural Notatio.pdf:application/pdf}
}

@inproceedings{calvo-zaragoza_recognition_2019,
	title = {Recognition of {Handwritten} {Music} {Symbols} using {Meta}-features {Obtained} from {Weak} {Classifiers} based on {Nearest} {Neighbor}},
	isbn = {978-989-758-222-6},
	url = {http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0006120200960104},
	abstract = {Digital Library},
	urldate = {2019-10-01},
	author = {Calvo-Zaragoza, Jorge and Valero-Mas, Jose Javier and Rico-Juan, Juan R.},
	month = oct,
	year = {2019},
	pages = {96--104},
	file = {Snapshot:/Users/emilyhopkins/Zotero/storage/EHL8ST3M/Link.html:text/html}
}

@inproceedings{calvo-zaragoza_pixelwise_2017,
	title = {Pixelwise classification for music document analysis},
	url = {http://cloud.simssa.ca/index.php/s/mVqSk1dMjhIsoMG},
	doi = {10.1109/IPTA.2017.8310134},
	abstract = {Content within musical documents not only contains music symbol but also include different elements such as staff lines, text, or frontispieces. Before attempting to automatically recognize components in these layers, it is necessary to perform an analysis of the musical documents in order to detect and classify each of these constituent parts. The obstacle for this analysis is the high heterogeneity amongst music collections, especially with ancient documents, which makes it difficult to devise methods that can be generalizable to a broader range of sources. In this paper we propose a data-driven document analysis framework based on machine learning that focuses on classifying regions of interest at pixel level. For that, we make use of Convolutional Neural Networks trained to infer the category of each pixel. The main advantage of this approach is that it can be applied regardless of the type of document provided, as long as training data is available. Since this work represents first efforts in that direction, our experimentation focuses on reporting a baseline classification using our framework. The experiments show promising performance, achieving an accuracy around 90\% in two corpora of old music documents.},
	booktitle = {2017 {Seventh} {International} {Conference} on {Image} {Processing} {Theory}, {Tools} and {Applications} ({IPTA})},
	author = {Calvo-Zaragoza, J. and Vigliensoni, G. and Fujinaga, I.},
	month = nov,
	year = {2017},
	keywords = {ancient documents, Convolutional Neural Network, convolutional neural networks, Convolutional neural networks, data-driven document analysis framework, Document Image Analysis, document image processing, image classification, Layout, learning (artificial intelligence), machine learning, Microsoft Windows, music, Music, Music Archives, music collections, music document analysis, music symbol, musical documents, neural nets, old music documents, Optical Music Recognition, Pixel classification, pixelwise classification, regions of interest classification, staff lines, Task analysis, Text analysis, Training},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:/Users/emilyhopkins/Zotero/storage/E6V3QCA9/8310134.html:text/html;IEEE Xplore Full Text PDF:/Users/emilyhopkins/Zotero/storage/YCGH9NPT/Calvo-Zaragoza et al. - 2017 - Pixelwise classification for music document analys.pdf:application/pdf}
}

@article{calvo-zaragoza_music_2016,
	title = {Music staff removal with supervised pixel classification},
	volume = {19},
	issn = {1433-2825},
	url = {https://doi.org/10.1007/s10032-016-0266-2},
	doi = {10.1007/s10032-016-0266-2},
	abstract = {This work presents a novel approach to tackle the music staff removal. This task is devoted to removing the staff lines from an image of a music score while maintaining the symbol information. It represents a key step in the performance of most optical music recognition systems. In the literature, staff removal is usually solved by means of image processing procedures based on the intrinsics of music scores. However, we propose to model the problem as a supervised learning classification task. Surprisingly, although there is a strong background and a vast amount of research concerning machine learning, the classification approach has remained unexplored for this purpose. In this context, each foreground pixel is labelled as either staff or symbol. We use pairs of scores with and without staff lines to train classification algorithms. We test our proposal with several well-known classification techniques. Moreover, in our experiments no attempt of tuning the classification algorithms has been made, but the parameters were set to the default setting provided by the classification software libraries. The aim of this choice is to show that, even with this straightforward procedure, results are competitive with state-of-the-art algorithms. In addition, we also discuss several advantages of this approach for which conventional methods are not applicable such as its high adaptability to any type of music score.},
	language = {en},
	number = {3},
	urldate = {2019-10-01},
	journal = {International Journal on Document Analysis and Recognition (IJDAR)},
	author = {Calvo-Zaragoza, Jorge and Micó, Luisa and Oncina, Jose},
	month = sep,
	year = {2016},
	keywords = {Optical music recognition, Pixel classification, Music staff removal, Supervised learning},
	pages = {211--219},
	file = {Springer Full Text PDF:/Users/emilyhopkins/Zotero/storage/444FBA2X/Calvo-Zaragoza et al. - 2016 - Music staff removal with supervised pixel classifi.pdf:application/pdf}
}

@inproceedings{calvo-zaragoza_document_2016,
	address = {New York, NY, USA},
	series = {{DLfM} 2016},
	title = {Document {Analysis} for {Music} {Scores} via {Machine} {Learning}},
	isbn = {978-1-4503-4751-8},
	url = {http://doi.acm.org/10.1145/2970044.2970047},
	doi = {10.1145/2970044.2970047},
	abstract = {Content within musical documents not only contains musical notation but can also include text, ornaments, annotations, and editorial data. Before any attempt at automatic recognition of elements in these layers, it is necessary to perform a document analysis process to detect and classify each of its constituent parts. The obstacle for this analysis is the high heterogeneity amongst collections, which makes it difficult to propose methods that can be generalizable to a broader range of sources. In this paper we propose a data-driven document analysis framework based on machine learning, which focuses on classifying regions of interest at pixel level. The main advantage of this approach is that it can be exploited regardless of the type of document provided, as long as training data is available. Our preliminary experimentation includes a set of specific tasks that can be performed on music such as the detection of staff lines, isolation of music symbols, and the layering of the document into its elemental parts.},
	urldate = {2019-10-01},
	booktitle = {Proceedings of the 3rd {International} {Workshop} on {Digital} {Libraries} for {Musicology}},
	publisher = {ACM},
	author = {Calvo-Zaragoza, Jorge and Vigliensoni, Gabriel and Fujinaga, Ichiro},
	year = {2016},
	note = {event-place: New York, USA},
	keywords = {Optical Music Recognition, Machine Learning, Document Analysis},
	pages = {37--40},
	file = {ACM Full Text PDF:/Users/emilyhopkins/Zotero/storage/L6DXDYNT/Calvo-Zaragoza et al. - 2016 - Document Analysis for Music Scores via Machine Lea.pdf:application/pdf}
}

@inproceedings{bell_approaches_2016,
	address = {New York, NY, USA},
	series = {{DLfM} 2016},
	title = {Approaches to {Handwritten} {Conductor} {Annotation} {Extraction} in {Musical} {Scores}},
	isbn = {978-1-4503-4751-8},
	url = {http://doi.acm.org/10.1145/2970044.2970053},
	doi = {10.1145/2970044.2970053},
	abstract = {Conductor copies of musical scores are typically rich in handwritten annotations. Ongoing archival efforts to digitize orchestral conductors' scores have made scanned copies of hundreds of these annotated scores available in digital formats. The extraction of handwritten annotations from digitized printed documents is a difficult task for computer vision, with most approaches focusing on the extraction of handwritten text. However, conductors' annotation practices provide us with at least two affordances, which make the task more tractable in the musical domain. First, many conductors opt to mark their scores using colored pencils, which contrast with the black and white print of sheet music. Consequently, we show promising results when using color separation techniques alone to recover handwritten annotations from conductors' scores. We also compare annotated scores to unannotated copies and use a printed sheet music comparison tool to recover handwritten annotations as additions to the clean copy. We then investigate the use of both of these techniques in a combined method, which improves the results of the color separation technique. These techniques are demonstrated using a sample of orchestral scores annotated by professional conductors of the New York Philharmonic. Handwritten annotation extraction in musical scores has applications to the systematic investigation of score annotation practices by performers, annotator attribution, and to the interactive presentation of annotated scores, which we briefly discuss.},
	urldate = {2019-10-01},
	booktitle = {Proceedings of the 3rd {International} {Workshop} on {Digital} {Libraries} for {Musicology}},
	publisher = {ACM},
	author = {Bell, Eamonn and Pugin, Laurent},
	year = {2016},
	note = {event-place: New York, USA},
	keywords = {annotation extraction, color clustering, conducting, image processing, image superimposition, orchestral scores},
	pages = {33--36},
	file = {ACM Full Text PDF:/Users/emilyhopkins/Zotero/storage/ISRASXR7/Bell and Pugin - 2016 - Approaches to Handwritten Conductor Annotation Ext.pdf:application/pdf}
}

@inproceedings{thomae_mensural_2019,
	address = {The Hague, Netherlands},
	title = {The {Mensural} {Scoring}-{Up} {Tool}},
	booktitle = {Proceedings of the 6th {International} {Workshop} on {Digital} {Libraries} for {Musicology}},
	publisher = {ACM},
	author = {Thomae, Martha E. and Cumming, Julie E. and Fujinaga, Ichiro},
	month = nov,
	year = {2019}
}

@inproceedings{gover_notation-based_2019,
	address = {The Hague, Netherlands},
	title = {A notation-based query language for searching in symbolic music},
	booktitle = {Proceedings of the 6th {International} {Workshop} on {Digital} {Libraries} for {Musicology}},
	publisher = {ACM},
	author = {Gover, Matan and Fujinaga, Ichiro},
	month = nov,
	year = {2019}
}

@inproceedings{napoles_lopez_key-finding_2019,
	address = {The Hague, Netherlands},
	title = {Key-{Finding} {Based} on a {Hidden} {Markov} {Model} and {Key} {Profiles}},
	booktitle = {Proceedings of the 6th {International} {Workshop} on {Digital} {Libraries} for {Musicology}},
	publisher = {ACM},
	author = {Nápoles López, Néstor and Arthur, Claire and Fujinaga, Ichiro},
	month = nov,
	year = {2019}
}

@inproceedings{de_reuse_pattern_2019,
	address = {Delft, Netherlands},
	title = {Pattern {Clustering} in {Monophonic} {Music} by {Learning} a {Non}-{Linear} {Embedding} from {Human} {Annotations}},
	booktitle = {Proceedings of the 20th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {de Reuse, Tim and Fujinaga, Ichiro},
	month = nov,
	year = {2019}
}

@inproceedings{pacha_learning_2019,
	title = {Learning {Notation} {Graph} {Construction} for {Full}-{Pipeline} {Optical} {Music} {Recognition}},
	booktitle = {Proceedings of the 20th {International} {Society} for {Music} {Information} {Retrieval} {Conference}},
	author = {Pacha, Alexander and Calvo-Zaragoza, Jorge and Hajič Jr., Jan},
	month = nov,
	year = {2019}
}

@inproceedings{de_reuse_robust_2019,
	address = {Delft, Netherlands},
	title = {Robust transcript alignment on medieval chant manuscripts},
	copyright = {All rights reserved},
	abstract = {We present a generalizable method of performing transcript alignment on the lyrics of medieval chant manuscripts. We use optical character recognition to generate a preliminary transcript of each page and then use a global sequence alignment method to match it up to the known correct transcript, combining the two incomplete sources of information into a high-quality alignment. We demonstrate this approach on manuscript pages using two different script styles from four different sources. This method requires little training data and works even when the transcript and the page itself have differing textual content, achieving per-syllable accuracies of 80–90\% across the four sources.},
	language = {en},
	booktitle = {Proceedings of the 2nd {International} {Workshop} on {Reading} {Music} {Systems}},
	author = {de Reuse, Timothy and Fujinaga, Ichiro},
	year = {2019}
}

@inproceedings{ju_interactive_2019,
	title = {An {Interactive} {Workflow} for {Generating} {Chord} {Labels} for {Homorhythmic} {Music} in {Symbolic} {Formats}},
	booktitle = {Proceedings of the 20th {International} {Society} for {Music} {Information} {Retrieval} {Conference}. {ISMIR}, {Delft}, {Netherlands}, np},
	author = {Ju, Yaolong and Howes, Samuel and McKay, Cory and Condit-Schultz, Nathaniel and Calvo-Zaragoza, Jorge and Fujinaga, Ichiro},
	year = {2019}
}